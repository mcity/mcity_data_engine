{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/v4.45.2/en/model_doc/grounding-dino\n",
    "https://huggingface.co/IDEA-Research/grounding-dino-tiny\n",
    "\n",
    "https://huggingface.co/docs/transformers/v4.45.2/en/model_doc/owlv2\n",
    "https://huggingface.co/google/owlv2-base-patch16-ensemble\n",
    "\n",
    "https://huggingface.co/docs/transformers/v4.45.2/en/model_doc/owlvit\n",
    "https://huggingface.co/docs/transformers/v4.45.2/en/model_doc/omdet-turbo\n",
    "\n",
    "https://huggingface.co/docs/transformers/main/model_doc/auto#transformers.AutoModelForZeroShotObjectDetection\n",
    "\n",
    "https://huggingface.co/docs/transformers/en/tasks/zero_shot_object_detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForZeroShotObjectDetection,\n",
    ")\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "tokens = {}\n",
    "with open(\"/home/dbogdoll/mcity_data_engine/.secret\", \"r\") as file:\n",
    "    for line in file:\n",
    "        key, value = line.strip().split(\"=\")\n",
    "        tokens[key] = value\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = tokens[\"HF_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "from utils.data_loader import FiftyOneTorchDatasetCOCO, TorchToHFDatasetCOCO\n",
    "\n",
    "try:\n",
    "    dataset_v51 = load_from_hub(\"dbogdollumich/mcity_fisheye_v51\")\n",
    "except:\n",
    "    dataset_v51 = fo.load_dataset(\"dbogdollumich/mcity_fisheye_v51\")\n",
    "pytorch_dataset = FiftyOneTorchDatasetCOCO(dataset_v51)\n",
    "pt_to_hf_converter = TorchToHFDatasetCOCO(pytorch_dataset)\n",
    "hf_dataset = pt_to_hf_converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/owlv2-base-patch16-ensemble\"\n",
    "texts = [\n",
    "    [\n",
    "        \"car\",\n",
    "        \"truck\",\n",
    "        \"bus\",\n",
    "        \"trailer\",\n",
    "        \"motorbike/cycler\",\n",
    "        \"pedestrian\",\n",
    "        \"van\",\n",
    "        \"pickup\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(config)\n",
    "print(processor)\n",
    "print(model)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training not yet implemented in HF, but maybe coming\n",
    "- https://github.com/huggingface/transformers/pull/34057/commits/a4f3d660b7ba9ac269c1e0870ea6e9048f72bdc0\n",
    "- https://github.com/huggingface/transformers/issues/33664\n",
    "- https://github.com/huggingface/transformers/issues/20091\n",
    "- https://github.com/stevebottos/owl-vit-object-detection\n",
    "- https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit#fine-tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "predictions_view = dataset_v51.take(16, seed=51)\n",
    "\n",
    "for sample in tqdm(predictions_view):\n",
    "    image = Image.open(sample.filepath)\n",
    "    inputs = processor(text=texts, images=image, return_tensors=\"pt\")\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get original image size\n",
    "    original_size = torch.Tensor([image.size[::-1]])\n",
    "\n",
    "    # Convert outputs (bounding boxes and class logits) to final bounding boxes and scores\n",
    "    results = processor.post_process_object_detection(\n",
    "        outputs=outputs, threshold=0.2, target_sizes=original_size\n",
    "    )\n",
    "    i = 0  # Retrieve predictions for the first image for the corresponding text queries\n",
    "    text = texts[i]\n",
    "    boxes, scores, labels = (\n",
    "        results[i][\"boxes\"],\n",
    "        results[i][\"scores\"],\n",
    "        results[i][\"labels\"],\n",
    "    )\n",
    "\n",
    "    # Convert to V51 format\n",
    "    # Convert to [top-left-x, top-left-y, width, height]\n",
    "    # in relative coordinates in [0, 1] x [0, 1]\n",
    "    width, height = image.size\n",
    "    detections = []\n",
    "    for box, score, label in zip(boxes, scores, labels):\n",
    "        top_left_x = box[0].item() / width\n",
    "        top_left_y = box[1].item() / height\n",
    "        box_width = (box[2] - box[0]).item() / width\n",
    "        box_height = (box[3] - box[1]).item() / height\n",
    "\n",
    "        detection = fo.Detection(\n",
    "            label=texts[0][label],\n",
    "            bounding_box=[top_left_x, top_left_y, box_width, box_height],\n",
    "            confidence=score.item(),\n",
    "        )\n",
    "        detections.append(detection)\n",
    "\n",
    "    sample[\"predictions\"] = fo.Detections(detections=detections)\n",
    "    sample.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = predictions_view.count_values(\"ground_truth.detections.label\")\n",
    "classes = sorted(counts, key=counts.get, reverse=True)[:10]\n",
    "\n",
    "results = predictions_view.evaluate_detections(\n",
    "    \"predictions\",\n",
    "    gt_field=\"ground_truth\",\n",
    "    eval_key=\"eval\",\n",
    "    compute_mAP=True,\n",
    ")\n",
    "results.print_report(classes=classes)\n",
    "print(\"mAP: \", results.mAP())\n",
    "\n",
    "# Print some statistics about the total TP/FP/FN counts\n",
    "print(\"TP: %d\" % predictions_view.sum(\"eval_tp\"))\n",
    "print(\"FP: %d\" % predictions_view.sum(\"eval_fp\"))\n",
    "print(\"FN: %d\" % predictions_view.sum(\"eval_fn\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = results.plot_pr_curves(classes=classes)\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = results.plot_confusion_matrix(classes=classes)\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(view=predictions_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = dataset_v51.default_classes\n",
    "id2label = {index: x for index, x in enumerate(categories, start=0)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    print(batch)\n",
    "    data = {}\n",
    "    data[\"pixel_values\"] = torch.stack([x[\"pixel_values\"] for x in batch])\n",
    "    data[\"labels\"] = [x[\"labels\"] for x in batch]\n",
    "    if \"pixel_mask\" in batch[0]:\n",
    "        data[\"pixel_mask\"] = torch.stack([x[\"pixel_mask\"] for x in batch])\n",
    "\n",
    "    input_ids = tokenizer(texts, padding=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    data[\"input_ids\"] = input_ids.repeat(len(batch), 1)  # Match batch size\n",
    "    return data\n",
    "\n",
    "\n",
    "def transform_batch(examples, image_processor, return_pixel_mask=False):\n",
    "    \"\"\"Apply format annotations in COCO format for object detection task\"\"\"\n",
    "\n",
    "    images = []\n",
    "    annotations = []\n",
    "\n",
    "    for image_path, annotation in zip(examples[\"image\"], examples[\"target\"]):\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image_np = np.array(image)\n",
    "        images.append(image_np)\n",
    "\n",
    "        # Annotation needs to be in COCO style annotation per bounding box\n",
    "        coco_annotations = []\n",
    "        for i, bbox in enumerate(annotation[\"bbox\"]):\n",
    "\n",
    "            # Convert bbox x_min, y_min, w, h to YOLO format x_center, y_center, w, h\n",
    "            bbox[0] = bbox[0] + bbox[2] / 2.0\n",
    "            bbox[1] = bbox[1] + bbox[3] / 2.0\n",
    "\n",
    "            # Ensure bbox values are within the expected range\n",
    "            assert all(0 <= coord <= 1 for coord in bbox), f\"Invalid bbox: {bbox}\"\n",
    "\n",
    "            coco_annotation = {\n",
    "                \"image_id\": annotation[\"image_id\"],\n",
    "                \"bbox\": bbox,\n",
    "                \"category_id\": annotation[\"category_id\"][i],\n",
    "                \"area\": annotation[\"area\"][i],\n",
    "                \"iscrowd\": 0,\n",
    "            }\n",
    "            coco_annotations.append(coco_annotation)\n",
    "        detr_annotation = {\n",
    "            \"image_id\": annotation[\"image_id\"],\n",
    "            \"annotations\": coco_annotations,\n",
    "        }\n",
    "        annotations.append(detr_annotation)\n",
    "\n",
    "    # Apply the image processor transformations: resizing, rescaling, normalization\n",
    "    result = image_processor(images=images, text=texts, return_tensors=\"pt\")\n",
    "\n",
    "    if not return_pixel_mask:\n",
    "        result.pop(\"pixel_mask\", None)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoProcessor,\n",
    "    AutoModelForZeroShotObjectDetection,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "from datasets import Split\n",
    "from functools import partial\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetuning the model on our data\n",
    "image_processor = AutoProcessor.from_pretrained(\n",
    "    model_name,\n",
    "    do_resize=False,\n",
    "    do_pad=False,  # Assumes all images have the same size\n",
    "    do_convert_annotations=False,  # expects YOLO (center_x, center_y, width, height) between [0,1]\n",
    ")\n",
    "\n",
    "hf_model_config = AutoConfig.from_pretrained(model_name)\n",
    "train_transform_batch = partial(transform_batch, image_processor=image_processor)\n",
    "validation_transform_batch = partial(transform_batch, image_processor=image_processor)\n",
    "\n",
    "hf_dataset[Split.TRAIN] = hf_dataset[Split.TRAIN].with_transform(train_transform_batch)\n",
    "hf_dataset[Split.VALIDATION] = hf_dataset[Split.VALIDATION].with_transform(\n",
    "    validation_transform_batch\n",
    ")\n",
    "\n",
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(\n",
    "    model_name,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    run_name=model_name,\n",
    "    output_dir=\"output/models/teacher/\" + model_name,\n",
    "    num_train_epochs=3,\n",
    "    fp16=False,\n",
    "    per_device_train_batch_size=8,\n",
    "    auto_find_batch_size=True,  # Automates the lowering process if CUDA OOM\n",
    "    dataloader_num_workers=8,\n",
    "    learning_rate=5e-05,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    weight_decay=0.0001,\n",
    "    max_grad_norm=0.01,\n",
    "    metric_for_best_model=\"eval_loss\",  # eval_map,\n",
    "    load_best_model_at_end=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    eval_do_concat_batches=False,\n",
    "    save_safetensors=False,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,\n",
    "    early_stopping_threshold=0.0,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=hf_dataset[Split.TRAIN],\n",
    "    eval_dataset=hf_dataset[Split.VALIDATION],\n",
    "    tokenizer=image_processor,\n",
    "    data_collator=collate_fn,\n",
    "    callbacks=[early_stopping_callback],\n",
    "    # compute_metrics=eval_compute_metrics_fn, # TODO Write eval function\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
