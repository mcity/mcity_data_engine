{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements to utilize all available GPUs for 1 dataset, n models, m GPUs:\n",
    "- Each model processes the entire dataset    \n",
    "- Dataset can be splitted for slow models    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DataParallel\n",
    "from torchvision import models, transforms\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "import math\n",
    "import time\n",
    "\n",
    "import logging\n",
    "\n",
    "import re\n",
    "\n",
    "import fiftyone as fo\n",
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "\n",
    "from utils.data_loader import FiftyOneTorchDatasetCOCO\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForObjectDetection,\n",
    "    AutoModelForZeroShotObjectDetection,\n",
    "    AutoProcessor,\n",
    "    BatchEncoding,\n",
    "    EarlyStoppingCallback,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading config file fiftyone.yml from dbogdollumich/mcity_fisheye_v51\n",
      "Loading dataset\n",
      "Importing samples...\n",
      " 100% |███████████████| 2744/2744 [87.8ms elapsed, 0s remaining, 31.2K samples/s]   \n"
     ]
    }
   ],
   "source": [
    "# Load dataset with V51 from HF\n",
    "with open(\"/home/dbogdoll/mcity_data_engine/.secret\", \"r\") as file:\n",
    "    for line in file:\n",
    "        key, value = line.strip().split(\"=\")\n",
    "        os.environ[key] = value\n",
    "\n",
    "try:\n",
    "    dataset_v51 = load_from_hub(\"dbogdollumich/mcity_fisheye_v51\")\n",
    "except:\n",
    "    dataset_v51 = fo.load_dataset(\"dbogdollumich/mcity_fisheye_v51\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SAMPLES = 130\n",
    "dataset_v51 = dataset_v51.take(MAX_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Voxel51 dataset: 100%|██████████| 130/130 [00:00<00:00, 271.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert to torch dataset\n",
    "dataset_torch = FiftyOneTorchDatasetCOCO(dataset_v51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _initialize_zero_shot_processor(hf_model_config, model_name, batch_size, object_classes, device\n",
    "    ):\n",
    "        processor, batch_classes, tokenized_text, batch_tasks = None, None, None, None\n",
    "        if type(hf_model_config).__name__ == \"GroundingDinoConfig\":\n",
    "            processor = AutoProcessor.from_pretrained(model_name)  # , do_rescale=False\n",
    "            # https://huggingface.co/docs/transformers/v4.45.2/en/model_doc/grounding-dino\n",
    "            classes = \" . \".join(object_classes) + \" . \"\n",
    "            batch_classes = [classes] * batch_size\n",
    "            tokenized_text = processor.tokenizer(\n",
    "                batch_classes,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=256,  # Adjust max_length to match vision hidden state\n",
    "            ).to(device)\n",
    "\n",
    "        elif type(hf_model_config).__name__ == \"Owlv2Config\":\n",
    "            processor = CustomOwlv2Processor.from_pretrained(model_name)\n",
    "            batch_classes = object_classes * batch_size\n",
    "            tokenized_text = processor.tokenizer(\n",
    "                batch_classes, padding=\"max_length\", return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "        elif type(hf_model_config).__name__ == \"OwlViTConfig\":\n",
    "            processor = AutoProcessor.from_pretrained(model_name)\n",
    "            batch_classes = object_classes * batch_size\n",
    "            tokenized_text = processor.tokenizer(\n",
    "                batch_classes, padding=\"max_length\", return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "        elif type(hf_model_config).__name__ == \"OmDetTurboConfig\":\n",
    "            processor = AutoProcessor.from_pretrained(model_name)\n",
    "            batch_classes = [object_classes] * batch_size\n",
    "            task = \"Detect {}.\".format(\", \".join(object_classes))\n",
    "            batch_tasks = [task] * batch_size\n",
    "        else:\n",
    "            logging.error(\n",
    "                \"HuggingFace AutoModel does not support \" + str(type(hf_model_config))\n",
    "            )\n",
    "\n",
    "        return processor, batch_classes, tokenized_text, batch_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _zero_shot_process_results(outputs):\n",
    "    # Process results\n",
    "    if type(hf_model_config).__name__ == \"GroundingDinoConfig\":\n",
    "        results = processor.post_process_grounded_object_detection(\n",
    "            outputs,\n",
    "            inputs.input_ids,\n",
    "            box_threshold=detection_threshold,\n",
    "            text_threshold=detection_threshold,\n",
    "        )\n",
    "    elif type(hf_model_config).__name__ in [\"Owlv2Config\", \"OwlViTConfig\"]:\n",
    "        results = processor.post_process_object_detection(\n",
    "            outputs=outputs,\n",
    "            threshold=detection_threshold,\n",
    "            target_sizes=target_sizes,\n",
    "        )\n",
    "    elif type(hf_model_config).__name__ == \"OmDetTurboConfig\":\n",
    "        results = processor.post_process_grounded_object_detection(\n",
    "            outputs,\n",
    "            classes=batch_classes,\n",
    "            score_threshold=detection_threshold,\n",
    "            nms_threshold=detection_threshold,\n",
    "            target_sizes=target_sizes,\n",
    "        )\n",
    "\n",
    "    # Store results in V51 dataset\n",
    "    for result, target in zip(results, targets):\n",
    "        boxes, scores = result[\"boxes\"], result[\"scores\"]\n",
    "\n",
    "        if \"labels\" in result:\n",
    "            labels = result[\"labels\"]\n",
    "        elif \"classes\" in result:  # OmDet deviates from the other models\n",
    "            labels = result[\"classes\"]\n",
    "\n",
    "        detections = []\n",
    "        for box, score, label in zip(boxes, scores, labels):\n",
    "            if type(hf_model_config).__name__ == \"GroundingDinoConfig\":\n",
    "                processed_label = label.split()[0]\n",
    "                if processed_label not in object_classes:\n",
    "                    matches = get_close_matches(\n",
    "                        processed_label, object_classes, n=1, cutoff=0.6\n",
    "                    )\n",
    "                    processed_label = matches[0] if matches else None\n",
    "                if processed_label == None:\n",
    "                    logging.info(\n",
    "                        \"Skipped detection with model \"\n",
    "                        + type(hf_model_config).__name__\n",
    "                        + \" due to unclear detection label: \"\n",
    "                        + label\n",
    "                    )\n",
    "                    continue\n",
    "                label = class_parts_dict[\n",
    "                    processed_label\n",
    "                ]  # Original label for eval\n",
    "                top_left_x = box[0].item()\n",
    "                top_left_y = box[1].item()\n",
    "                box_width = (box[2] - box[0]).item()\n",
    "                box_height = (box[3] - box[1]).item()\n",
    "\n",
    "            elif type(hf_model_config).__name__ in [\n",
    "                \"Owlv2Config\",\n",
    "                \"OwlViTConfig\",\n",
    "            ]:\n",
    "                label = class_parts_dict[object_classes[label]]\n",
    "                top_left_x = box[0].item() / img_width\n",
    "                top_left_y = box[1].item() / img_height\n",
    "                box_width = (box[2].item() - box[0].item()) / img_width\n",
    "                box_height = (box[3].item() - box[1].item()) / img_height\n",
    "            elif type(hf_model_config).__name__ == \"OmDetTurboConfig\":\n",
    "                label = class_parts_dict[label]\n",
    "                top_left_x = box[0].item() / img_width\n",
    "                top_left_y = box[1].item() / img_height\n",
    "                box_width = (box[2].item() - box[0].item()) / img_width\n",
    "                box_height = (box[3].item() - box[1].item()) / img_height\n",
    "\n",
    "            detection = fo.Detection(\n",
    "                label=label,\n",
    "                bounding_box=[\n",
    "                    top_left_x,\n",
    "                    top_left_y,\n",
    "                    box_width,\n",
    "                    box_height,\n",
    "                ],\n",
    "                confidence=score.item(),\n",
    "            )\n",
    "            detection[\"bbox_area\"] = (\n",
    "                detection[\"bounding_box\"][2] * detection[\"bounding_box\"][3]\n",
    "            )\n",
    "            detections.append(detection)\n",
    "\n",
    "        # Attach label to V51 dataset\n",
    "        sample = dataset_v51[target[\"image_id\"]]\n",
    "        sample[pred_key] = fo.Detections(detections=detections)\n",
    "        sample.save()\n",
    "\n",
    "def _zero_shot_inference(\n",
    "        data_loader,\n",
    "        model_name,\n",
    "        device,\n",
    "        progress_counter,\n",
    "        batch_size=16,\n",
    "        detection_threshold=0.2,\n",
    "        object_classes=[None],\n",
    "    ):\n",
    "        print(f'Launched process with {device}')\n",
    "        pred_key = re.sub(r\"[\\W-]+\", \"_\", \"pred_\" + model_name)\n",
    "        eval_key = re.sub(r\"[\\W-]+\", \"_\", \"eval_\" + model_name)\n",
    "\n",
    "        writer = SummaryWriter(log_dir=\"logs/tensorboard/teacher_zeroshot\")\n",
    "        hf_model_config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "        # Process combined label types like \"motorbike/cycler\"\n",
    "        processed_classes = [\n",
    "            part for classname in object_classes for part in classname.split(\"/\")\n",
    "        ]\n",
    "        class_parts_dict = {\n",
    "            part: classname\n",
    "            for classname in object_classes\n",
    "            for part in classname.split(\"/\")\n",
    "        }\n",
    "        object_classes = processed_classes\n",
    "\n",
    "        processor, batch_classes, tokenized_text, batch_tasks = (\n",
    "            _initialize_zero_shot_processor(\n",
    "                hf_model_config=hf_model_config,\n",
    "                model_name=model_name,\n",
    "                batch_size=batch_size,\n",
    "                object_classes=object_classes,\n",
    "                device=device,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        model = AutoModelForZeroShotObjectDetection.from_pretrained(model_name).to(device)\n",
    "\n",
    "        for step, (images, targets) in enumerate(data_loader):\n",
    "            print(f\"Started batch {step} for GPU {device}\")\n",
    "            start_time = time.time()\n",
    "            if len(images) != batch_size:  # For final batch, if batch not full\n",
    "                processor, batch_classes, tokenized_text, batch_tasks = (\n",
    "                    _initialize_zero_shot_processor(\n",
    "                        hf_model_config,\n",
    "                        model_name,\n",
    "                        len(images),  # Key difference\n",
    "                        object_classes,\n",
    "                        device,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            target_sizes = [tuple(img.shape[1:]) for img in images]\n",
    "            # FIXME Assumption that all images have the same size\n",
    "            img_height = target_sizes[0][0]\n",
    "            img_width = target_sizes[0][1]  \n",
    "            for target_size in target_sizes:\n",
    "                if target_size[0] != img_height or target_size[1] != img_width:\n",
    "                    logging.error(f\"Not all images have the same size. Current w/h {target_size[1]},{target_size[0]} conflict with {img_width},{img_height}\")\n",
    "            \n",
    "            if type(hf_model_config).__name__ == \"OmDetTurboConfig\":\n",
    "                images = [to_pil_image(image) for image in images]\n",
    "            else:\n",
    "                images = [(image).to(device, non_blocking=True) for image in images]\n",
    "\n",
    "            # Process inputs\n",
    "            if type(hf_model_config).__name__ == \"GroundingDinoConfig\":\n",
    "                inputs = processor(\n",
    "                    text=None, images=images, return_tensors=\"pt\"\n",
    "                ).to(device)\n",
    "                inputs.update(tokenized_text)\n",
    "\n",
    "            elif type(hf_model_config).__name__ == \"Owlv2Config\":\n",
    "                inputs = processor(\n",
    "                    text=None, images=images, return_tensors=\"pt\"\n",
    "                ).to(device, non_blocking=True)\n",
    "                inputs.update(tokenized_text)\n",
    "            elif type(hf_model_config).__name__ == \"OwlViTConfig\":\n",
    "                inputs = processor(\n",
    "                    text=batch_classes, images=images, return_tensors=\"pt\"\n",
    "                ).to(device)\n",
    "                # inputs.update(tokenized_text)\n",
    "            elif type(hf_model_config).__name__ == \"OmDetTurboConfig\":\n",
    "                inputs = processor(\n",
    "                    text=batch_classes,\n",
    "                    images=images,\n",
    "                    task=batch_tasks,\n",
    "                    return_tensors=\"pt\",\n",
    "                ).to(device)\n",
    "\n",
    "            # Model inference\n",
    "            print(f\"Finished pre-processing of batch {step} for GPU {device}\")\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs)\n",
    "            print(f\"Finished model inference of batch {step} for GPU {device}\")\n",
    "\n",
    "            _zero_shot_process_results(outputs)\n",
    "            print(f\"Finished post-processing of batch {step} for GPU {device}\")\n",
    "\n",
    "            # Log inference performance\n",
    "            end_time = time.time()\n",
    "            batch_duration = end_time - start_time\n",
    "            batches_per_second = 1 / batch_duration\n",
    "            frames_per_second = batches_per_second * batch_size\n",
    "            writer.add_scalar(\n",
    "                f\"inference/{device}/frames_per_second\", frames_per_second, step\n",
    "            )\n",
    "\n",
    "            # Update the progress counter\n",
    "            with progress_counter.get_lock():  # Ensure thread safety for counter update\n",
    "                progress_counter.value += 1\n",
    "            tqdm.write(f\"Progress {device}: {progress_counter:.2f}%\")\n",
    "            writer.add_scalar(\n",
    "                f\"inference/total_steps\", progress_counter, step\n",
    "            )\n",
    "\n",
    "        # Store labels https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.export\n",
    "        dataset_v51.export(\n",
    "            export_dir=\"output/testmultigpu/\",\n",
    "            dataset_type=fo.types.COCODetectionDataset,\n",
    "            data_path=\"data.json\",\n",
    "            export_media=None,  # \"manifest\",\n",
    "            label_field=pred_key,\n",
    "            progress=True,\n",
    "        )\n",
    "\n",
    "        writer.close()\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset and launch\n",
    "models_dict = {\n",
    "    \"IDEA-Research/grounding-dino-tiny\": {\n",
    "        \"dataset_splits\": 2,\n",
    "        \"batch_size\": 4\n",
    "    },\n",
    "    \"omlab/omdet-turbo-swin-tiny-hf\": {\n",
    "        \"dataset_splits\": 1,\n",
    "        \"batch_size\": 8\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create dataset subsets for models with splits\n",
    "dataset_length = len(dataset_torch)\n",
    "subsets = {}\n",
    "\n",
    "for model_name, config in models_dict.items():\n",
    "    if config[\"dataset_splits\"] > 1:\n",
    "        split_size = dataset_length // config[\"dataset_splits\"]\n",
    "        subsets[model_name] = [\n",
    "            Subset(dataset_torch, list(range(i * split_size, (i + 1) * split_size)))\n",
    "            for i in range(config[\"dataset_splits\"])\n",
    "        ]\n",
    "        # Handle any remainder\n",
    "        if dataset_length % config[\"dataset_splits\"] > 0:\n",
    "            subsets[model_name][-1] = Subset(\n",
    "                dataset_torch, list(range((config[\"dataset_splits\"] - 1) * split_size, dataset_length))\n",
    "            )\n",
    "    else:\n",
    "        subsets[model_name] = [dataset_torch]  # Entire dataset for models with dataset_splits=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    run = None\n",
    "    try:\n",
    "        run = wandb.init(\n",
    "        name=\"Multi GPU Teacher\",\n",
    "        allow_val_change=True,\n",
    "        sync_tensorboard=True,\n",
    "        project=\"Teacher Dev\",)\n",
    "\n",
    "        # Device assignment (manually assigning GPUs)\n",
    "        if torch.cuda.is_available():\n",
    "            devices = [torch.device(f\"cuda:{i}\") for i in range(torch.cuda.device_count())]\n",
    "            n_gpus = len(devices)\n",
    "            print(f\"Using {n_gpus} GPUs: {devices}\")\n",
    "        else:\n",
    "            print(\"Multiprocessing not possible, CUDA not avaiable.\")\n",
    "        \n",
    "        # Run inference for each model and its subsets\n",
    "        NUM_WORKERS = 32\n",
    "        n_workers_per_gpu = math.floor(NUM_WORKERS / n_gpus)\n",
    "\n",
    "        object_classes = [\"pedestrian\", \"cyclist\", \"vehicle\"]\n",
    "\n",
    "        processes = []\n",
    "        assigned_gpu_index = 0\n",
    "\n",
    "        # Shared progress counter (using Value to be shared between processes)\n",
    "        progress_counter = mp.Value('i', 0)  # Shared integer for progress tracking\n",
    "\n",
    "        for model_idx, (model_name, config) in enumerate(models_dict.items()):\n",
    "            subset_list = subsets[model_name]\n",
    "            \n",
    "            for subset_idx, subset in enumerate(subset_list):\n",
    "                dataloader = DataLoader(\n",
    "                    subset,\n",
    "                    batch_size=config[\"batch_size\"],\n",
    "                    num_workers=n_workers_per_gpu,\n",
    "                    pin_memory=True,\n",
    "                    collate_fn=lambda batch: list(zip(*batch)),\n",
    "                    shuffle=False\n",
    "                )\n",
    "\n",
    "                assigned_device = devices[assigned_gpu_index]\n",
    "                assigned_gpu_index += 1\n",
    "                print(f\"Running {model_name} on {assigned_device} with {len(subset)} samples (subset {subset_idx + 1})...\")\n",
    "                p = mp.Process(target=_zero_shot_inference, args=(subset, model_name, assigned_device, progress_counter, 16, 0.2, object_classes))\n",
    "                processes.append(p)\n",
    "                \n",
    "        # Start all processes\n",
    "        for p in processes:\n",
    "            p.start()\n",
    "\n",
    "        # Wait for all processes to finish\n",
    "        for p in processes:\n",
    "            p.join()\n",
    "        \n",
    "        run.finish(exit_code=0)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        if run:\n",
    "            run.finish(exit_code=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dbogdoll/mcity_data_engine/scripts/wandb/run-20241119_120859-i4g805o8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mcity/Teacher%20Dev/runs/i4g805o8' target=\"_blank\">Multi GPU Teacher</a></strong> to <a href='https://wandb.ai/mcity/Teacher%20Dev' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mcity/Teacher%20Dev' target=\"_blank\">https://wandb.ai/mcity/Teacher%20Dev</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mcity/Teacher%20Dev/runs/i4g805o8' target=\"_blank\">https://wandb.ai/mcity/Teacher%20Dev/runs/i4g805o8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 3 GPUs: [device(type='cuda', index=0), device(type='cuda', index=1), device(type='cuda', index=2)]\n",
      "Running IDEA-Research/grounding-dino-tiny on cuda:0 with 65 samples (subset 1)...\n",
      "Running IDEA-Research/grounding-dino-tiny on cuda:1 with 65 samples (subset 2)...\n",
      "Running omlab/omdet-turbo-swin-tiny-hf on cuda:2 with 130 samples (subset 1)...\n",
      "Launched process with cuda:0\n",
      "Launched process with cuda:1\n",
      "Launched process with cuda:2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[132], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[129], line 55\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# Wait for all processes to finish\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m processes:\n\u001b[0;32m---> 55\u001b[0m         \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     run\u001b[38;5;241m.\u001b[39mfinish(exit_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/sw/pkgs/arc/python/3.11.5/lib/python3.11/multiprocessing/process.py:149\u001b[0m, in \u001b[0;36mBaseProcess.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_pid \u001b[38;5;241m==\u001b[39m os\u001b[38;5;241m.\u001b[39mgetpid(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a child process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a started process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 149\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_popen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     _children\u001b[38;5;241m.\u001b[39mdiscard(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/sw/pkgs/arc/python/3.11.5/lib/python3.11/multiprocessing/popen_fork.py:43\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWNOHANG\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode\n",
      "File \u001b[0;32m/sw/pkgs/arc/python/3.11.5/lib/python3.11/multiprocessing/popen_fork.py:27\u001b[0m, in \u001b[0;36mPopen.poll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m         pid, sts \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mwaitpid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid, flag)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;66;03m# Child process not yet created. See #1731717\u001b[39;00m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;66;03m# e.errno == errno.ECHILD == 10\u001b[39;00m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
