{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image, ImageDraw\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"omlab/omdet-turbo-swin-tiny-hf\")\n",
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(\n",
    "    \"omlab/omdet-turbo-swin-tiny-hf\"\n",
    ")\n",
    "\n",
    "classes = [\"cat\", \"remote\", \"car\", \"truck\", \"bus\", \"traffic light\"]\n",
    "task = \"Detect {}.\".format(\", \".join(classes))\n",
    "\n",
    "url1 = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image1 = Image.open(BytesIO(requests.get(url1).content)).convert(\"RGB\")\n",
    "\n",
    "\n",
    "url2 = \"http://images.cocodataset.org/train2017/000000257813.jpg\"\n",
    "image2 = Image.open(BytesIO(requests.get(url2).content)).convert(\"RGB\")\n",
    "\n",
    "url3 = \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\"\n",
    "image3 = Image.open(BytesIO(requests.get(url3).content)).convert(\"RGB\")\n",
    "\n",
    "image4 = Image.open(\n",
    "    \"/home/dbogdoll/mcity_data_engine/scripts/fisheye_small.png\"\n",
    ").convert(\"RGB\")\n",
    "image5 = Image.open(\n",
    "    \"/home/dbogdoll/mcity_data_engine/scripts/fisheye_test.png\"\n",
    ").convert(\"RGB\")\n",
    "\n",
    "images = [image1, image2, image3, image4, image5]\n",
    "batch_classes = [classes] * len(images)\n",
    "batch_task = [task] * len(images)\n",
    "print(batch_task)\n",
    "print(batch_classes)\n",
    "print(batch_task)\n",
    "\n",
    "for image in images:\n",
    "    # Print image information\n",
    "    print(f\"Image size: {image.size}\")\n",
    "    print(f\"Image mode: {image.mode}\")\n",
    "    print(f\"Image format: {image.format}\")\n",
    "\n",
    "inputs = processor(\n",
    "    images=images,\n",
    "    text=batch_classes,\n",
    "    task=batch_task,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "print(inputs)\n",
    "with torch.amp.autocast(\"cuda\"):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "print(outputs)\n",
    "# convert outputs (bounding boxes and class logits)\n",
    "target_sizes = [img.size[::-1] for img in images]\n",
    "print(target_sizes)\n",
    "results = processor.post_process_grounded_object_detection(\n",
    "    outputs,\n",
    "    classes=batch_classes,\n",
    "    target_sizes=target_sizes,\n",
    "    score_threshold=0.2,\n",
    "    nms_threshold=0.3,\n",
    ")\n",
    "print(results)\n",
    "for i, result in enumerate(results):\n",
    "    for score, class_name, box in zip(\n",
    "        result[\"scores\"], result[\"classes\"], result[\"boxes\"]\n",
    "    ):\n",
    "        box = [round(i, 1) for i in box.tolist()]\n",
    "        print(\n",
    "            f\"Detected {class_name} with confidence \"\n",
    "            f\"{round(score.item(), 2)} at location {box} in image {i}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw bounding boxes on the image\n",
    "i = 4\n",
    "image = images[i]\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"classes\"]\n",
    "\n",
    "\n",
    "for box, label in zip(boxes, labels):\n",
    "    draw.rectangle(box.tolist(), outline=\"red\", width=2)\n",
    "    draw.text((box[0], box[1]), label, fill=\"red\")\n",
    "\n",
    "# Display the image\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "tokens = {}\n",
    "with open(\"/home/dbogdoll/mcity_data_engine/.secret\", \"r\") as file:\n",
    "    for line in file:\n",
    "        key, value = line.strip().split(\"=\")\n",
    "        tokens[key] = value\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = tokens[\"HF_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import fiftyone as fo\n",
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "from utils.data_loader import FiftyOneTorchDatasetCOCO, TorchToHFDatasetCOCO\n",
    "\n",
    "try:\n",
    "    dataset_v51 = load_from_hub(\"dbogdollumich/mcity_fisheye_v51\", max_samples=32)\n",
    "except:\n",
    "    dataset_v51 = fo.load_dataset(\"dbogdollumich/mcity_fisheye_v51\")\n",
    "pytorch_dataset = FiftyOneTorchDatasetCOCO(dataset_v51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "model_name = \"omlab/omdet-turbo-swin-tiny-hf\"\n",
    "\n",
    "batch_size = 2\n",
    "pred_key = \"predictions_omdet\"\n",
    "eval_key = \"eval\"\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "pytorch_dataset = FiftyOneTorchDatasetCOCO(\n",
    "    dataset_v51,\n",
    "    # transforms=transform,\n",
    ")\n",
    "data_loader = DataLoader(\n",
    "    pytorch_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    collate_fn=lambda x: list(zip(*x)),\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "classes_v51 = dataset_v51.default_classes\n",
    "\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "batch_classes = [classes_v51] * batch_size\n",
    "task = \"Detect {}.\".format(\", \".join(classes_v51))\n",
    "\n",
    "batch_tasks = [task] * batch_size\n",
    "print(classes_v51)\n",
    "print(batch_classes)\n",
    "print(batch_tasks)\n",
    "\n",
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(model_name).to(device)\n",
    "\n",
    "for step, (images, targets) in enumerate(\n",
    "    tqdm(data_loader, desc=\"Zero Shot Teacher Model \" + model_name)\n",
    "):\n",
    "\n",
    "    for image in images:\n",
    "        # Print image information\n",
    "        print(f\"Image size: {image.size}\")\n",
    "        print(f\"Image mode: {image.mode}\")\n",
    "        print(f\"Image format: {image.format}\")\n",
    "\n",
    "    target_sizes = [img.size[::-1] for img in images]\n",
    "\n",
    "    # target_sizes = [\n",
    "    #    tuple(img.shape[1:]) for img in images\n",
    "    # ]  # style [(480,640),(480,640)] h,w\n",
    "    # images = [(image).to(device, non_blocking=True) for image in images]\n",
    "\n",
    "    print(images)\n",
    "    print(target_sizes)\n",
    "\n",
    "    inputs = processor(\n",
    "        text=batch_classes,\n",
    "        images=images,\n",
    "        task=batch_tasks,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "\n",
    "    print(inputs)\n",
    "    with torch.amp.autocast(\"cuda\"):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "    print(outputs)\n",
    "    results = processor.post_process_grounded_object_detection(\n",
    "        outputs,\n",
    "        classes=batch_classes,\n",
    "        score_threshold=0.2,\n",
    "        nms_threshold=0.3,\n",
    "        target_sizes=target_sizes,\n",
    "    )\n",
    "    print(results)\n",
    "    for result, target in zip(results, targets):\n",
    "        boxes, scores, labels = result[\"boxes\"], result[\"scores\"], result[\"classes\"]\n",
    "\n",
    "        detections = []\n",
    "        for box, score, label in zip(boxes, scores, labels):\n",
    "            # Get image size\n",
    "            img_path = pytorch_dataset.img_paths[\n",
    "                target[\"image_id\"].item()\n",
    "            ]  # ID is stored in annotation\n",
    "            sample = dataset_v51[img_path]\n",
    "            img_width = sample.metadata.width\n",
    "            img_height = sample.metadata.height\n",
    "\n",
    "            label = label\n",
    "            # Convert bbox to V51 type\n",
    "            top_left_x = box[0].item() / img_width\n",
    "            top_left_y = box[1].item() / img_height\n",
    "            box_width = (box[2].item() - box[0].item()) / img_width\n",
    "            box_height = (box[3].item() - box[1].item()) / img_height\n",
    "\n",
    "            detection = fo.Detection(\n",
    "                label=label,\n",
    "                bounding_box=[top_left_x, top_left_y, box_width, box_height],\n",
    "                confidence=score.item(),\n",
    "            )\n",
    "            detections.append(detection)\n",
    "\n",
    "        img_path = pytorch_dataset.img_paths[\n",
    "            target[\"image_id\"].item()\n",
    "        ]  # ID is stored in annotation\n",
    "        sample = dataset_v51[img_path]\n",
    "        sample[pred_key] = fo.Detections(detections=detections)\n",
    "        sample.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset_v51)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
