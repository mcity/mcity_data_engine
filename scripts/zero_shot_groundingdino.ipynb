{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image, ImageDraw\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = {}\n",
    "with open(\"/home/dbogdoll/mcity_data_engine/.secret\", \"r\") as file:\n",
    "    for line in file:\n",
    "        key, value = line.strip().split(\"=\")\n",
    "        tokens[key] = value\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = tokens[\"HF_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "from utils.data_loader import FiftyOneTorchDatasetCOCO, TorchToHFDatasetCOCO\n",
    "\n",
    "\n",
    "try:\n",
    "    dataset_v51 = load_from_hub(\"dbogdollumich/mcity_fisheye_v51\", max_samples=16)\n",
    "except:\n",
    "    dataset_v51 = fo.load_dataset(\"dbogdollumich/mcity_fisheye_v51\")\n",
    "pytorch_dataset = FiftyOneTorchDatasetCOCO(dataset_v51)\n",
    "pt_to_hf_converter = TorchToHFDatasetCOCO(pytorch_dataset)\n",
    "hf_dataset = pt_to_hf_converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def zeroshot_collate_fn(batch):\n",
    "    return list(zip(*batch))\n",
    "\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    pytorch_dataset,\n",
    "    batch_size=2,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    collate_fn=zeroshot_collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(data_loader))\n",
    "print(batch)\n",
    "item = batch[0]\n",
    "print(item)\n",
    "image = item[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    \"car\",\n",
    "    \"truck\",\n",
    "    \"bus\",\n",
    "    \"trailer\",\n",
    "    \"motorbike/cycler\",\n",
    "    \"pedestrian\",\n",
    "    \"van\",\n",
    "    \"pickup\",\n",
    "]\n",
    "\n",
    "processed_classes = [part for classname in classes for part in classname.split(\"/\")]\n",
    "class_parts_dict = {\n",
    "    part: classname for classname in classes for part in classname.split(\"/\")\n",
    "}\n",
    "\n",
    "classes = processed_classes\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"IDEA-Research/grounding-dino-tiny\"\n",
    "device = \"cuda\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(device)\n",
    "\n",
    "text = \". \".join(classes) + \".\"\n",
    "print(text)\n",
    "\n",
    "tokenized_text = processor.tokenizer(text, return_tensors=\"pt\")\n",
    "print(tokenized_text)\n",
    "inputs = processor(images=image, text=text, return_tensors=\"pt\").to(device)\n",
    "print(inputs)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "target_size = [tuple(image.shape[1:])]\n",
    "\n",
    "results = processor.post_process_grounded_object_detection(\n",
    "    outputs,\n",
    "    inputs.input_ids,\n",
    "    box_threshold=0.2,\n",
    "    text_threshold=0.2,\n",
    "    target_sizes=target_size\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'results' contains the bounding boxes and labels\n",
    "boxes = results[0][\"boxes\"]\n",
    "labels = results[0][\"labels\"]\n",
    "\n",
    "image_pil = T.ToPILImage()(image)\n",
    "\n",
    "# Draw bounding boxes on the image\n",
    "draw = ImageDraw.Draw(image_pil)\n",
    "for box, label in zip(boxes, labels):\n",
    "    draw.rectangle(box.tolist(), outline=\"red\", width=2)\n",
    "    draw.text((box[0], box[1]), label, fill=\"red\")\n",
    "\n",
    "# Display the image\n",
    "display(image_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_tokenized_text = [tokenized_text] * data_loader.batch_size\n",
    "\n",
    "print(batch_tokenized_text)\n",
    "batch_text = [text] * data_loader.batch_size\n",
    "print(batch_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_text = [text] * data_loader.batch_size\n",
    "print(batch_text)\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "for images, targets in tqdm(data_loader):\n",
    "    inputs = processor(text=batch_text, images=images, return_tensors=\"pt\").to(device)\n",
    "    print(\"Inputs shape (first run):\", {k: v.shape for k, v in inputs.items()})\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    results = processor.post_process_grounded_object_detection(\n",
    "        outputs,\n",
    "        inputs.input_ids,\n",
    "        box_threshold=0.2,\n",
    "    )\n",
    "\n",
    "tokenized_texts = processor.tokenizer(\n",
    "    batch_text,\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=256,  # Adjust max_length to match vision hidden state\n",
    ").to(device)\n",
    "print(\"Tokenized texts shape:\", {k: v.shape for k, v in tokenized_texts.items()})\n",
    "\n",
    "for images, targets in tqdm(data_loader):\n",
    "    inputs = processor(text=None, images=images, return_tensors=\"pt\").to(device)\n",
    "    inputs.update(tokenized_texts)\n",
    "    print(\"Inputs shape (second run):\", {k: v.shape for k, v in inputs.items()})\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    results = processor.post_process_grounded_object_detection(\n",
    "        outputs,\n",
    "        inputs.input_ids,\n",
    "        box_threshold=0.2,\n",
    "    )\n",
    "    for result in results:\n",
    "        boxes, scores, labels = (\n",
    "            result[\"boxes\"],\n",
    "            result[\"scores\"],\n",
    "            result[\"labels\"],\n",
    "        )\n",
    "        print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "tokens = {}\n",
    "with open(\"/home/dbogdoll/mcity_data_engine/.secret\", \"r\") as file:\n",
    "    for line in file:\n",
    "        key, value = line.strip().split(\"=\")\n",
    "        tokens[key] = value\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = tokens[\"HF_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "from utils.data_loader import FiftyOneTorchDatasetCOCO, TorchToHFDatasetCOCO\n",
    "\n",
    "try:\n",
    "    dataset_v51 = load_from_hub(\"dbogdollumich/mcity_fisheye_v51\")\n",
    "except:\n",
    "    dataset_v51 = fo.load_dataset(\"dbogdollumich/mcity_fisheye_v51\")\n",
    "pytorch_dataset = FiftyOneTorchDatasetCOCO(dataset_v51)\n",
    "pt_to_hf_converter = TorchToHFDatasetCOCO(pytorch_dataset)\n",
    "hf_dataset = pt_to_hf_converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "predictions_view = dataset_v51.take(16, seed=51)\n",
    "\n",
    "for sample in tqdm(predictions_view):\n",
    "    image = Image.open(sample.filepath)\n",
    "    inputs = processor(text=text, images=image, return_tensors=\"pt\").to(device)\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get original image size\n",
    "    original_size = torch.Tensor([image.size[::-1]])\n",
    "\n",
    "    # Convert outputs (bounding boxes and class logits) to final bounding boxes and scores\n",
    "    results = processor.post_process_grounded_object_detection(\n",
    "        outputs, inputs.input_ids, box_threshold=0.2, text_threshold=0.2\n",
    "    )\n",
    "    for result in results:\n",
    "        boxes, scores, labels = (\n",
    "            result[\"boxes\"],\n",
    "            result[\"scores\"],\n",
    "            result[\"labels\"],\n",
    "        )\n",
    "        print(len(labels))\n",
    "\n",
    "        # Convert to V51 format\n",
    "        # Convert to [top-left-x, top-left-y, width, height]\n",
    "        # in relative coordinates in [0, 1] x [0, 1]\n",
    "        width, height = image.size\n",
    "        detections = []\n",
    "        for box, score, label in zip(boxes, scores, labels):\n",
    "            top_left_x = box[0].item()\n",
    "            top_left_y = box[1].item()\n",
    "            box_width = (box[2] - box[0]).item()\n",
    "            box_height = (box[3] - box[1]).item()\n",
    "\n",
    "            detection = fo.Detection(\n",
    "                label=label,\n",
    "                bounding_box=[top_left_x, top_left_y, box_width, box_height],\n",
    "                confidence=score.item(),\n",
    "            )\n",
    "            detections.append(detection)\n",
    "\n",
    "    sample[\"predictions\"] = fo.Detections(detections=detections)\n",
    "    sample.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(view=predictions_view)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
