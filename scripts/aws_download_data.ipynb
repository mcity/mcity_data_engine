{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import datetime\n",
    "import pytz\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import wandb\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import shutil\n",
    "import json\n",
    "import base64\n",
    "import fiftyone as fo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime.datetime(2023, 11, 19)    # year, month, day   \n",
    "end_date = datetime.datetime(2023, 11, 25)      # 25 November 2023: Michigan vs Ohio\n",
    "cameras_excluded = []\n",
    "\n",
    "# Path to download the data\n",
    "storage_target_root = \"/media/dbogdoll/Datasets/data_engine_rolling\"\n",
    "data_target = os.path.join(storage_target_root, \"data\")\n",
    "os.makedirs(data_target, exist_ok=True)\n",
    "\n",
    "log_target = os.path.join(storage_target_root, \"logs\")\n",
    "os.makedirs(log_target, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to AWS\n",
    "with open(\"/home/dbogdoll/mcity_data_engine/.secret\", \"r\") as file:\n",
    "    for line in file:\n",
    "        key, value = line.strip().split(\"=\")\n",
    "        os.environ[key] = value\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    aws_access_key_id=os.environ.get(\"AWS_ACCESS_KEY_ID\", None),\n",
    "    aws_secret_access_key=os.environ.get(\"AWS_SECRET_ACCESS_KEY\", None),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 24 cameras\n"
     ]
    }
   ],
   "source": [
    "cameras = {\n",
    "    \"Geddes_Huron_1\",\n",
    "    \"Geddes_Huron_2\",\n",
    "    \"Huron_Plymouth_1\",\n",
    "    \"Huron_Plymouth_2\",\n",
    "    \"Main_stadium_1\",\n",
    "    \"Main_stadium_2\",\n",
    "    \"Plymouth_Beal\",\n",
    "    \"Plymouth_Bishop\",\n",
    "    \"Plymouth_EPA\",\n",
    "    \"Plymouth_Georgetown\",\n",
    "    \"State_Ellsworth_NE\",\n",
    "    \"State_Ellsworth_NW\",\n",
    "    \"State_Ellsworth_SE\",\n",
    "    \"State_Ellsworth_SW\",\n",
    "    \"Fuller_Fuller_CT\",\n",
    "    \"Fuller_Glazier_1\",\n",
    "    \"Fuller_Glazier_2\",\n",
    "    \"Fuller_Glen\",\n",
    "    \"Dexter_Maple_1\",\n",
    "    \"Dexter_Maple_2\",\n",
    "    \"Hubbard_Huron_1\",\n",
    "    \"Hubbard_Huron_2\",\n",
    "    \"Maple_Miller_1\",\n",
    "    \"Maple_Miller_2\",    \n",
    "}\n",
    "\n",
    "cameras_dict = {camera.lower(): {} for camera in cameras}\n",
    "\n",
    "for id, camera in enumerate(cameras_dict):\n",
    "    if camera not in cameras_excluded:\n",
    "        cameras_dict[camera][\"id\"] = id\n",
    "        cameras_dict[camera][\"aws-sources\"] = {}\n",
    "\n",
    "print(f\"Processed {len(cameras_dict)} cameras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_aws_result(result):\n",
    "    if 'CommonPrefixes' in result:\n",
    "        folders = [prefix['Prefix'] for prefix in result['CommonPrefixes']] # Get list of folders from AWS response\n",
    "        return folders\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing AWS sources: 100%|██████████| 2/2 [00:00<00:00,  6.54it/s]\n"
     ]
    }
   ],
   "source": [
    "aws_sources = {\n",
    "    \"sip-sensor-data\":[\"\"],\n",
    "    \"sip-sensor-data2\":[\"wheeler1/\",\"wheeler2/\"],\n",
    "}\n",
    "\n",
    "for bucket in tqdm(aws_sources, desc=\"Processing AWS sources\"):\n",
    "    for folder in aws_sources[bucket]:\n",
    "        # Get and pre-process AWS data\n",
    "        result = s3.list_objects_v2(Bucket=bucket, Prefix=folder, Delimiter='/')\n",
    "        folders = process_aws_result(result)\n",
    "\n",
    "        #Align folder names with camera names\n",
    "        folders_aligned = [re.sub(r'(?<!_)(\\d)', r'_\\1', folder.lower().rstrip('/')) for folder in folders] # Align varying AWS folder names with camera names\n",
    "        folders_aligned = [folder.replace('fullerct', 'fuller_ct') for folder in folders_aligned]  # Replace \"fullerct\" with \"fuller_ct\" to align with camera names\n",
    "        folders_aligned = [folder.replace('fullser', 'fuller') for folder in folders_aligned]  # Fix \"fullser\" typo in AWS\n",
    "\n",
    "        # Check cameras for AWS sources\n",
    "        if folders:\n",
    "            for camera_name in cameras_dict:\n",
    "                for folder_name, folder_name_aligned in zip(folders, folders_aligned):\n",
    "                    if camera_name in folder_name_aligned and \"gs_\" in folder_name_aligned: # gs_ is the prefix used in AWS\n",
    "                        aws_source = f\"{bucket}/{folder_name}\"\n",
    "                        if aws_source not in cameras_dict[camera_name][\"aws-sources\"]:\n",
    "                            cameras_dict[camera_name][\"aws-sources\"][aws_source] = {}\n",
    "        else:\n",
    "            print(f\"AWS did not return a list of folders for {bucket}/{folder}\")\n",
    "            print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_data_test():\n",
    "    \"Select a single test file to download for each AWS source\"\n",
    "    n_cameras = 0\n",
    "    n_aws_sources = 0\n",
    "    n_files_to_download = 0\n",
    "    download_size_bytes = 0\n",
    "    for camera in cameras_dict:\n",
    "        n_cameras += 1\n",
    "        for aws_source in cameras_dict[camera][\"aws-sources\"]:\n",
    "            n_aws_sources += 1\n",
    "            file_downloaded_test = False\n",
    "            bucket = aws_source.split(\"/\")[0]\n",
    "            prefix_camera = \"/\".join(aws_source.split(\"/\")[1:])\n",
    "            result = s3.list_objects_v2(Bucket=bucket, Prefix=prefix_camera, Delimiter='/')\n",
    "            folders_day = process_aws_result(result)        # Each folder represents a day\n",
    "            for folder_day in folders_day:\n",
    "                date = folder_day.split('/')[-2]\n",
    "                cameras_dict[camera][\"aws-sources\"][aws_source][date] = {}\n",
    "                result = s3.list_objects_v2(Bucket=bucket, Prefix=folder_day, Delimiter='/')\n",
    "                folders_hour = process_aws_result(result)    # Each folder represents an hour\n",
    "                for folder_hour in folders_hour:\n",
    "                    result = s3.list_objects_v2(Bucket=bucket, Prefix=folder_hour, Delimiter='/')\n",
    "                    files = result[\"Contents\"]\n",
    "                    for file in files:\n",
    "                        n_files_to_download += 1\n",
    "                        download_size_bytes += file[\"Size\"]\n",
    "                        file_name = os.path.basename(file[\"Key\"])\n",
    "                        cameras_dict[camera][\"aws-sources\"][aws_source][date][file_name] = {}\n",
    "                        cameras_dict[camera][\"aws-sources\"][aws_source][date][file_name][\"key\"] = file[\"Key\"]\n",
    "                        cameras_dict[camera][\"aws-sources\"][aws_source][date][file_name][\"size\"] = file[\"Size\"]\n",
    "                        cameras_dict[camera][\"aws-sources\"][aws_source][date][file_name][\"date\"] = file[\"LastModified\"].strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                        file_downloaded_test = True\n",
    "                        print(f\"{aws_source} : {file_name}\")\n",
    "                        break # for file in files:\n",
    "                    if file_downloaded_test:\n",
    "                        break   # for folder_hour in folders_hour:\n",
    "                if file_downloaded_test:\n",
    "                    break   # for folder_day in folders_day:\n",
    "\n",
    "    log_file_path = os.path.join(\"cameras_dict.json\")\n",
    "    with open(log_file_path, 'w') as json_file:\n",
    "        json.dump(cameras_dict, json_file, indent=4)  \n",
    "\n",
    "    print(f\"Found {n_cameras} cameras\")\n",
    "    print(f\"Found {n_aws_sources} AWS sources\")\n",
    "    print(f\"Found {n_files_to_download} files to download\")\n",
    "    return n_files_to_download, download_size_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_data():\n",
    "    n_files_to_download = 0\n",
    "    download_size_bytes = 0\n",
    "    for camera in tqdm(cameras_dict, desc=\"Looking for data entries in range\"):\n",
    "        for aws_source in cameras_dict[camera][\"aws-sources\"]:\n",
    "            bucket = aws_source.split(\"/\")[0]\n",
    "            prefix_camera = \"/\".join(aws_source.split(\"/\")[1:])\n",
    "            result = s3.list_objects_v2(Bucket=bucket, Prefix=prefix_camera, Delimiter='/')\n",
    "            folders_day = process_aws_result(result)        # Each folder represents a day\n",
    "            for folder_day in folders_day:\n",
    "                date = folder_day.split('/')[-2]\n",
    "                timestamp = datetime.datetime.strptime(date, '%Y-%m-%d')\n",
    "                if start_date <= timestamp <= end_date:     # Only collect data within the date range\n",
    "                    cameras_dict[camera][\"aws-sources\"][aws_source][date] = {}\n",
    "                    result = s3.list_objects_v2(Bucket=bucket, Prefix=folder_day, Delimiter='/')\n",
    "                    folders_hour = process_aws_result(result)    # Each folder represents an hour\n",
    "                    for folder_hour in folders_hour:\n",
    "                        result = s3.list_objects_v2(Bucket=bucket, Prefix=folder_hour, Delimiter='/')\n",
    "                        files = result[\"Contents\"]\n",
    "                        for file in files:\n",
    "                            n_files_to_download += 1\n",
    "                            download_size_bytes += file[\"Size\"]\n",
    "                            file_name = os.path.basename(file[\"Key\"])\n",
    "                            cameras_dict[camera][\"aws-sources\"][aws_source][date][file_name] = {}\n",
    "                            cameras_dict[camera][\"aws-sources\"][aws_source][date][file_name][\"key\"] = file[\"Key\"]\n",
    "                            cameras_dict[camera][\"aws-sources\"][aws_source][date][file_name][\"size\"] = file[\"Size\"]\n",
    "                            cameras_dict[camera][\"aws-sources\"][aws_source][date][file_name][\"date\"] = file[\"LastModified\"].strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    print(f\"Found {n_files_to_download} files to download\")\n",
    "    return n_files_to_download, download_size_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sip-sensor-data/gs_Plymouth_Bishop/ : sip-delivery-stream-1-2023-03-13-18-56-11-ad8348dc-4dd4-3f8c-9403-add27be0c8b8\n",
      "sip-sensor-data2/wheeler1/gs_Plymouth_Bishop/ : sip-data-stream2-delivery-stream-2-2024-04-04-18-39-25-01d61059-af25-3c57-bbc6-d2bb2fb2e9bf\n",
      "sip-sensor-data2/wheeler2/gs_Maple_Miller_1/ : sip-data-stream2-delivery-stream-2-2024-04-12-17-23-12-b55b9f37-364a-39c2-803d-ab58bb86f711\n",
      "sip-sensor-data2/wheeler1/gs_Fullser_FullerCT/ : sip-data-stream2-delivery-stream-2-2024-04-12-16-08-52-b39d2153-114e-3f15-8e67-66651d357768\n",
      "sip-sensor-data/gs_State_Ellsworth_SE/ : sip-delivery-stream-1-2023-08-30-20-10-15-c22c084d-a7f2-3cbc-96b4-3d2b5401fff3\n",
      "sip-sensor-data2/wheeler1/gs_State_Ellsworth_SE/ : sip-data-stream2-delivery-stream-2-2024-03-07-18-14-02-80bb8ed5-f4c5-39fb-b68f-4ba535a64aa9\n",
      "sip-sensor-data2/wheeler2/gs_Fuller_Glen/ : sip-data-stream2-delivery-stream-2-2024-04-12-17-23-24-790869bc-fb2c-3526-9db1-da080c40697d\n",
      "sip-sensor-data/gs_Main_stadium2/ : sip-delivery-stream-1-2023-03-13-19-04-50-242cf314-4437-365e-9579-759daea3840e\n",
      "sip-sensor-data2/wheeler1/gs_Main_Stadium_2/ : sip-data-stream2-delivery-stream-2-2024-03-07-18-14-05-01a3dd5f-b826-3a49-9861-2833f9e46bf4\n",
      "sip-sensor-data2/wheeler2/gs_Maple_Miller_2/ : sip-data-stream2-delivery-stream-2-2024-04-12-17-23-13-fd863ea0-9a5a-3b62-a76f-27c832a6d566\n",
      "sip-sensor-data2/wheeler2/gs_Dexter_Maple_2/ : sip-data-stream2-delivery-stream-2-2024-04-12-17-23-13-36a9f88e-31dc-3528-b88a-064edc7964f3\n",
      "sip-sensor-data/gs_Huron_Plymouth2/ : sip-delivery-stream-1-2023-03-13-19-04-50-754bbf4b-d0bf-3a29-b83c-3cab1b75d2e0\n",
      "sip-sensor-data2/wheeler1/gs_Huron_Plymouth_2/ : sip-data-stream2-delivery-stream-2-2024-04-12-16-08-52-040ada6a-dfda-365f-9509-34b0151670ef\n",
      "sip-sensor-data2/wheeler1/gs_Huron_plymouth_2/ : sip-data-stream2-delivery-stream-2-2024-04-04-18-55-39-70eaaeed-8bba-3ec5-b834-a3fd0aeb9f07\n",
      "sip-sensor-data/gs_Plymouth_Georgetown/ : sip-delivery-stream-1-2023-03-13-18-56-11-23966e69-4352-3b11-badb-49a985dd0570\n",
      "sip-sensor-data2/wheeler1/gs_Plymouth_Georgetown/ : sip-data-stream2-delivery-stream-2-2024-04-04-18-39-24-98c2d5a7-0dea-3c81-b58d-3bce6999d179\n",
      "sip-sensor-data/gs_State_Ellsworth_NW/ : sip-delivery-stream-1-2023-08-30-20-10-16-87884c94-581e-35c7-92d5-b736caa8e77f\n",
      "sip-sensor-data2/wheeler1/gs_State_Ellsworth_NW/ : sip-data-stream2-delivery-stream-2-2024-03-07-18-13-57-0dc57636-cadb-3fbc-8a97-75060fab0b21\n",
      "sip-sensor-data/gs_Huron_Plymouth1/ : sip-delivery-stream-1-2023-03-13-19-04-50-95d2c432-ded2-31e9-a82d-b7a0a6833f08\n",
      "sip-sensor-data/gs_Plymouth_EPA/ : sip-delivery-stream-1-2023-03-15-14-15-15-0127be5b-7480-3886-a20f-9032462a29bc\n",
      "sip-sensor-data2/wheeler1/gs_Plymouth_EPA/ : sip-data-stream2-delivery-stream-2-2024-04-04-18-39-24-6037ff71-05e9-32ed-96d6-71dbcf543f09\n",
      "sip-sensor-data2/wheeler1/gs_Fullser_Glazier_1/ : sip-data-stream2-delivery-stream-2-2024-04-12-16-08-52-8bcf63c8-e46b-3fc9-8855-50eaf824bd39\n",
      "sip-sensor-data/gs_State_Ellsworth_SW/ : sip-delivery-stream-1-2023-08-30-20-10-16-c43bb59e-7545-30d3-b52e-ef1b76fcc340\n",
      "sip-sensor-data2/wheeler1/gs_State_Ellsworth_SW/ : sip-data-stream2-delivery-stream-2-2024-03-07-18-13-56-3b988640-6ec0-3513-bbbe-4129231a52b6\n",
      "sip-sensor-data2/wheeler2/gs_Dexter_Maple_1/ : sip-data-stream2-delivery-stream-2-2024-04-12-17-23-13-c6b502c3-b8ef-3073-8851-96ba890eecb0\n",
      "sip-sensor-data/gs_Plymouth_Beal/ : sip-delivery-stream-1-2023-03-13-18-56-11-1d6c8acc-5073-33f9-ab61-d0d12715541f\n",
      "sip-sensor-data2/wheeler1/gs_Plymouth_Beal/ : sip-data-stream2-delivery-stream-2-2024-04-04-18-26-39-1f41670f-12e0-3191-bb7e-e0fbd5387423\n",
      "sip-sensor-data/gs_State_Ellsworth_NE/ : sip-delivery-stream-1-2023-08-30-19-58-03-44c128f1-726a-3ea5-bcaf-5b73d4d4a32c\n",
      "sip-sensor-data2/wheeler1/gs_State_Ellsworth_NE/ : sip-data-stream2-delivery-stream-2-2024-03-07-17-36-56-73993be5-03fe-3cdd-baf2-1c46c6fc400e\n",
      "sip-sensor-data2/wheeler1/gs_Fullser_Glazier_2/ : sip-data-stream2-delivery-stream-2-2024-04-12-16-08-52-670f5995-71af-38b6-a4ec-51576fb06129\n",
      "sip-sensor-data/gs_Geddes_Huron2/ : sip-delivery-stream-1-2023-03-13-19-00-17-0ceadbca-8b3d-3cd0-91bb-1141c2974284\n",
      "sip-sensor-data2/wheeler1/gs_Geddes_Huron_2/ : sip-data-stream2-delivery-stream-2-2024-04-04-19-15-17-26516487-0151-3b8d-8772-fa319f0dfa0d\n",
      "sip-sensor-data/gs_Main_stadium1/ : sip-delivery-stream-1-2023-03-13-19-04-50-086f8c5c-288d-32c3-b405-cc351d20d68c\n",
      "sip-sensor-data2/wheeler1/gs_Main_Stadium_1/ : sip-data-stream2-delivery-stream-2-2024-03-07-18-13-55-83378470-fa8c-38a8-aca8-03b75f2b15fd\n",
      "sip-sensor-data2/wheeler2/gs_Hubbard_Huron_1/ : sip-data-stream2-delivery-stream-2-2024-04-13-09-53-57-ad853fd7-6d06-374d-b8f8-f8309b309944\n",
      "sip-sensor-data2/wheeler2/gs_Hubbard_Huron_2/ : sip-data-stream2-delivery-stream-2-2024-04-13-09-53-43-a3a817e6-5f60-36c9-b4f8-fe27dfb9ee2c\n",
      "sip-sensor-data/gs_Geddes_Huron1/ : sip-delivery-stream-1-2023-03-13-19-00-17-c5616dfa-3d70-3ed7-9862-6693d63c8b70\n",
      "sip-sensor-data2/wheeler1/gs_Geddes_Huron_1/ : sip-data-stream2-delivery-stream-2-2024-04-04-19-15-14-5ee29861-7822-3438-a9a1-7dfe123878cb\n",
      "Found 24 cameras\n",
      "Found 38 AWS sources\n",
      "Found 38 files to download\n"
     ]
    }
   ],
   "source": [
    "n_files_to_download, download_size_bytes = select_data_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size 0.0030873126233927906 TB is within 1.5 TB limit\n",
      "Date range of 7 days is within 7 days limit\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def safety_checks():\n",
    "    # Safety checks prior to downloading\n",
    "    passed_checks = True\n",
    "\n",
    "    # Check if each camera was assigned at least one AWS source\n",
    "    for camera in cameras_dict:\n",
    "        if len(cameras_dict[camera][\"aws-sources\"]) == 0:\n",
    "            print(f\"Camera {camera} has no AWS sources\")\n",
    "            passed_checks = False\n",
    "\n",
    "    # Check if the total size of the data to download is within the limit\n",
    "    MAX_SIZE_TB = 1.5\n",
    "    total_size_tb = download_size_bytes / (1024 ** 4)\n",
    "    if total_size_tb > MAX_SIZE_TB:\n",
    "        print(f\"Total size {total_size_tb} TB exceeds {MAX_SIZE_TB} TB limit\")\n",
    "        passed_checks = False\n",
    "    else:\n",
    "        print(f\"Total size {total_size_tb} TB is within {MAX_SIZE_TB} TB limit\")\n",
    "\n",
    "    # Check if the date range is within the limit\n",
    "    MAX_DAYS = 7\n",
    "    days_delta = (end_date - start_date).days + 1 # +1 to include the end date\n",
    "    if (days_delta) > MAX_DAYS:\n",
    "        print(f\"Date range of {days_delta} days exceeds {MAX_DAYS} days limit\")\n",
    "        passed_checks = False\n",
    "    else:\n",
    "        print(f\"Date range of {days_delta} days is within {MAX_DAYS} days limit\")\n",
    "    return passed_checks\n",
    "\n",
    "passed_checks = safety_checks()\n",
    "print(passed_checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dbogdoll/mcity_data_engine/logs/wandb/wandb/run-20241114_130556-l1g8bnap</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mcity/Data%20Engine%20Download/runs/l1g8bnap' target=\"_blank\">20231119_to_20231125</a></strong> to <a href='https://wandb.ai/mcity/Data%20Engine%20Download' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mcity/Data%20Engine%20Download' target=\"_blank\">https://wandb.ai/mcity/Data%20Engine%20Download</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mcity/Data%20Engine%20Download/runs/l1g8bnap' target=\"_blank\">https://wandb.ai/mcity/Data%20Engine%20Download/runs/l1g8bnap</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 38/38 [01:41<00:00,  2.67s/it]\n",
      "Checking downloaded data: 100%|██████████| 24/24 [00:00<00:00, 162622.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>download/mb_per_second</td><td>▆▆▆▆▆▁▅▅▁▆▆▆▇▇▅█▃▁▄▂▂▅▅▁▇▃▇▄▅▇▆▂▆▁▇▇▄▂</td></tr><tr><td>global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>download/mb_per_second</td><td>9.96038</td></tr><tr><td>global_step</td><td>38</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">20231119_to_20231125</strong> at: <a href='https://wandb.ai/mcity/Data%20Engine%20Download/runs/l1g8bnap' target=\"_blank\">https://wandb.ai/mcity/Data%20Engine%20Download/runs/l1g8bnap</a><br/> View project at: <a href='https://wandb.ai/mcity/Data%20Engine%20Download' target=\"_blank\">https://wandb.ai/mcity/Data%20Engine%20Download</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/home/dbogdoll/mcity_data_engine/logs/wandb/wandb/run-20241114_130556-l1g8bnap/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Download data\n",
    "def download_data(delete_old_data=False):\n",
    "    mb_per_s_list = []\n",
    "    \n",
    "    if passed_checks:\n",
    "        download_successful = True\n",
    "\n",
    "        if delete_old_data:\n",
    "            try:\n",
    "                shutil.rmtree(data_target)\n",
    "            except:\n",
    "                pass\n",
    "            os.makedirs(data_target) \n",
    "\n",
    "        step = 0\n",
    "        writer = SummaryWriter(log_dir=\"logs/download/s3\")\n",
    "        download_started = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        with tqdm(desc=\"Downloading data\", total=n_files_to_download) as pbar:\n",
    "            for camera in cameras_dict:\n",
    "                for aws_source in cameras_dict[camera][\"aws-sources\"]:\n",
    "                    bucket = aws_source.split(\"/\", 1)[0]\n",
    "                    for date in cameras_dict[camera][\"aws-sources\"][aws_source]:\n",
    "                        for file in cameras_dict[camera][\"aws-sources\"][aws_source][date]:\n",
    "                            time_start = time.time()\n",
    "                            \n",
    "                            # AWS S3 Download\n",
    "                            file_name = os.path.basename(file)\n",
    "                            key = cameras_dict[camera][\"aws-sources\"][aws_source][date][file_name][\"key\"]\n",
    "                            target = os.path.join(data_target, file_name)\n",
    "                            s3.download_file(bucket, key, target)\n",
    "                            \n",
    "                            # Calculate duration and GB/s\n",
    "                            file_size_mb = cameras_dict[camera][\"aws-sources\"][aws_source][date][file_name][\"size\"] / (1024 ** 2)\n",
    "                            time_end = time.time()\n",
    "                            duration = time_end - time_start\n",
    "                            mb_per_s = file_size_mb / duration\n",
    "                            mb_per_s_list.append(mb_per_s)\n",
    "                            \n",
    "                            # Update stats\n",
    "                            step += 1\n",
    "                            pbar.update(1)\n",
    "                            writer.add_scalar(\"download/mb_per_second\", mb_per_s, step)\n",
    "        writer.close()\n",
    "\n",
    "        # Check if all files were downloaded properly\n",
    "        downloaded_files = os.listdir(data_target)\n",
    "\n",
    "        for camera in tqdm(cameras_dict, desc=\"Checking downloaded data\"):\n",
    "            for aws_source in cameras_dict[camera][\"aws-sources\"]:\n",
    "                for date in cameras_dict[camera][\"aws-sources\"][aws_source]:\n",
    "                    for file in cameras_dict[camera][\"aws-sources\"][aws_source][date]:\n",
    "                        file_name = os.path.basename(file)\n",
    "                        if file not in downloaded_files:\n",
    "                            download_successful = False\n",
    "                            cameras_dict[camera][\"aws-sources\"][aws_source][date][file_name][\"download_successful\"] = False\n",
    "                            print(f\"File {file} was not downloaded properly\")\n",
    "                        else:\n",
    "                            cameras_dict[camera][\"aws-sources\"][aws_source][date][file_name][\"download_successful\"] = True\n",
    "        \n",
    "        if download_successful: # Save log only if data is fully downloaded\n",
    "            log = {}\n",
    "            log[\"selection_start_date\"] = start_date.strftime(\"%Y-%m-%d\")\n",
    "            log[\"selection_end_date\"] = end_date.strftime(\"%Y-%m-%d\")\n",
    "            log[\"old_files_deleted\"] = delete_old_data\n",
    "            log[\"n_files_to_download\"] = n_files_to_download\n",
    "            log[\"download_size_tb\"] = download_size_bytes / (1024 ** 4)\n",
    "            log[\"download_started\"] = download_started\n",
    "            log[\"download_ended\"] = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            log[\"download_speed_avg_mbs\"] = sum(mb_per_s_list) / len(mb_per_s_list)\n",
    "            log[\"data\"] = cameras_dict\n",
    "            log_name = download_started.replace(\" \", \"_\").replace(\":\", \"_\") + \".json\"\n",
    "            log_file_path = os.path.join(log_target, log_name)\n",
    "            with open(log_file_path, 'w') as json_file:\n",
    "                json.dump(log, json_file, indent=4)  \n",
    "        else:\n",
    "            print(\"Download failed\")\n",
    "    else:\n",
    "        download_successful = False\n",
    "        print(\"Safety checks failed. Not downloading data\")\n",
    "\n",
    "    return download_successful\n",
    "\n",
    "run = wandb.init(\n",
    "    name=f\"{start_date.strftime('%Y%m%d')}_to_{end_date.strftime('%Y%m%d')}\",\n",
    "    sync_tensorboard=True,\n",
    "    group=\"S3\",\n",
    "    job_type=\"download\",\n",
    "    project=\"Data Engine Download\",\n",
    ")\n",
    "\n",
    "download_successful = download_data(delete_old_data=True)\n",
    "print(download_successful)\n",
    "run.finish(exit_code=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_v51_metadata():\n",
    "    sample_fields = [\n",
    "        {\n",
    "            \"name\": \"timestamp\",\n",
    "            \"ftype\": \"fiftyone.core.fields.DateTimeField\",\n",
    "            \"embedded_doc_type\": None,\n",
    "            \"subfield\": None,\n",
    "            \"fields\": [],\n",
    "            \"db_field\": \"timestamp\",\n",
    "            \"description\": None,\n",
    "            \"info\": None,\n",
    "            \"read_only\": True,\n",
    "            \"created_at\": {\n",
    "                \"$date\": \"2024-11-14T15:36:12.758Z\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"sensor\",\n",
    "            \"ftype\": \"fiftyone.core.fields.StringField\",\n",
    "            \"embedded_doc_type\": None,\n",
    "            \"subfield\": None,\n",
    "            \"fields\": [],\n",
    "            \"db_field\": \"sensor\",\n",
    "            \"description\": None,\n",
    "            \"info\": None,\n",
    "            \"read_only\": True,\n",
    "            \"created_at\": {\n",
    "                \"$date\": \"2024-11-14T15:36:12.758Z\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    v51_metadata = {}\n",
    "    v51_metadata[\"name\"] = \"Data Engine Rolling Dataset\"  \n",
    "    #v51_metadata[\"sample_fields\"] = sample_fields\n",
    "\n",
    "    file_path = os.path.join(storage_target_root, \"metadata.json\")\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(v51_metadata, json_file)\n",
    "\n",
    "set_v51_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unpacking data: 100%|██████████| 24/24 [00:05<00:00,  4.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# Unpack data\n",
    "# Support V51 dataset structure https://docs.voxel51.com/user_guide/dataset_creation/datasets.html#fiftyonedataset\n",
    "\n",
    "def unpack_data():\n",
    "    v51_samples = {}\n",
    "    v51_samples_array = []\n",
    "    if download_successful:\n",
    "        for camera in tqdm(cameras_dict, desc=\"Unpacking data\"):\n",
    "            for aws_source in cameras_dict[camera][\"aws-sources\"]:\n",
    "                for date in cameras_dict[camera][\"aws-sources\"][aws_source]:\n",
    "                    for file in cameras_dict[camera][\"aws-sources\"][aws_source][date]:\n",
    "                        file_path = os.path.join(data_target, file)\n",
    "                        unpacking_successful = True\n",
    "                        with open(file_path, 'r') as file:\n",
    "                            for line in file:\n",
    "                                try:\n",
    "                                    data = json.loads(line)\n",
    "                                    v51_sample = {}\n",
    "                                    if \"time\" in data and \"data\" in data:\n",
    "                                        # Get data\n",
    "                                        timestamp = data.get(\"time\")\n",
    "                                        image_base64 = data.get(\"data\")\n",
    "\n",
    "                                        # Get timestamp\n",
    "                                        time_obj = datetime.datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S.%f')\n",
    "                                        formatted_time = time_obj.strftime('%Y-%m-%dT%H:%M:%S.%f')[:-3] + 'Z'\n",
    "                                        \n",
    "                                    elif \"image\" in data and \"sensor_name\" in data and \"event_timestamp\" in data:\n",
    "                                        # Get data\n",
    "                                        image_base64 = data.get(\"image\")\n",
    "                                        sensor_name = data.get(\"sensor_name\")\n",
    "                                        timestamp = data.get(\"event_timestamp\")\n",
    "                                        \n",
    "                                        # Get timestamps in UTC and Michigan time\n",
    "                                        utc_time = datetime.datetime.fromtimestamp(timestamp, tz=datetime.timezone.utc)\n",
    "                                        michigan_tz = pytz.timezone('America/Detroit')\n",
    "                                        michigan_time = utc_time.astimezone(michigan_tz)\n",
    "                                        formatted_time = michigan_time.strftime('%Y-%m-%dT%H:%M:%S.%f')[:-3] + 'Z'\n",
    "                                        # TODO Make sure that the conversion to Michigan time is correct\n",
    "                                    else:\n",
    "                                        unpacking_successful = False\n",
    "                                        print(f\"Format cannot be processed: {data}\")\n",
    "                                        continue\n",
    "                                    \n",
    "                                    if image_base64 and formatted_time:\n",
    "                                        # Decode the base64 image data\n",
    "                                        image_data = base64.b64decode(image_base64)\n",
    "\n",
    "                                        # File paths\n",
    "                                        image_filename = f\"{camera}_{formatted_time}.jpg\"\n",
    "                                        output_path = os.path.join(data_target, image_filename)\n",
    "\n",
    "                                        # Prepare import with V51\n",
    "                                        v51_sample[\"filepath\"] = output_path\n",
    "                                        v51_sample[\"sensor\"] = camera\n",
    "                                        # Ensure milliseconds are in 3 digits\n",
    "                                        milliseconds = formatted_time.split('.')[1][:3].ljust(3, '0')\n",
    "                                        formatted_time = formatted_time.split('.')[0] + '.' + milliseconds + 'Z'\n",
    "                                        iso8601_regex = re.compile(r'^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}(\\.\\d{3})?Z$')\n",
    "                                        iso8601_conform = bool(iso8601_regex.match(formatted_time))\n",
    "                                        if not iso8601_conform:\n",
    "                                            print(f\"Timestamp does not conform to ISO8601: {formatted_time}\")\n",
    "                                        v51_sample[\"timestamp\"] = {\"$date\": formatted_time}\n",
    "                                        v51_samples_array.append(v51_sample) \n",
    "\n",
    "                                        # Save the decoded image data as a JPEG\n",
    "                                        with open(output_path, 'wb') as image_file:\n",
    "                                            image_file.write(image_data)\n",
    "                                    else:\n",
    "                                        unpacking_successful = False\n",
    "                                        print(f\"There was an issue during file processing of {file}\")\n",
    "                                        continue\n",
    "\n",
    "                                except json.JSONDecodeError as e:\n",
    "                                    unpacking_successful = False\n",
    "                                    print(f\"Error decoding JSON: {e}\")\n",
    "                        \n",
    "                        # Delete the original file if unpacking was successful\n",
    "                        #if unpacking_successful:\n",
    "                        #    os.remove(file_path)\n",
    "\n",
    "    # Store V51 metadata files\n",
    "    v51_samples[\"samples\"] = v51_samples_array\n",
    "    file_path = os.path.join(storage_target_root, \"samples.json\")\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(v51_samples, json_file)\n",
    "\n",
    "unpack_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing samples...\n",
      " 100% |█████████████| 27096/27096 [306.1ms elapsed, 0s remaining, 88.9K samples/s]   \n",
      "Name:        Data Engine Rolling Dataset\n",
      "Media type:  None\n",
      "Num samples: 27096\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:               fiftyone.core.fields.ObjectIdField\n",
      "    filepath:         fiftyone.core.fields.StringField\n",
      "    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n",
      "    created_at:       fiftyone.core.fields.DateTimeField\n",
      "    last_modified_at: fiftyone.core.fields.DateTimeField\n",
      "    sensor:           fiftyone.core.fields.StringField\n",
      "    timestamp:        fiftyone.core.fields.DateTimeField\n",
      "[<Sample: {\n",
      "    'id': '673643dcdfd70ada14caee50',\n",
      "    'media_type': None,\n",
      "    'filepath': '/media/dbogdoll/Datasets/data_engine_rolling/data/plymouth_bishop_2023-03-13T14:56:11.419Z.jpg',\n",
      "    'tags': [],\n",
      "    'metadata': None,\n",
      "    'created_at': datetime.datetime(2024, 11, 14, 18, 39, 24, 355000),\n",
      "    'last_modified_at': datetime.datetime(2024, 11, 14, 18, 39, 24, 355000),\n",
      "    'sensor': 'plymouth_bishop',\n",
      "    'timestamp': datetime.datetime(2023, 3, 13, 14, 56, 11, 419000),\n",
      "}>, <Sample: {\n",
      "    'id': '673643dcdfd70ada14caee51',\n",
      "    'media_type': None,\n",
      "    'filepath': '/media/dbogdoll/Datasets/data_engine_rolling/data/plymouth_bishop_2023-03-13T14:56:11.402Z.jpg',\n",
      "    'tags': [],\n",
      "    'metadata': None,\n",
      "    'created_at': datetime.datetime(2024, 11, 14, 18, 39, 24, 355000),\n",
      "    'last_modified_at': datetime.datetime(2024, 11, 14, 18, 39, 24, 355000),\n",
      "    'sensor': 'plymouth_bishop',\n",
      "    'timestamp': datetime.datetime(2023, 3, 13, 14, 56, 11, 402000),\n",
      "}>, <Sample: {\n",
      "    'id': '673643dcdfd70ada14caee52',\n",
      "    'media_type': None,\n",
      "    'filepath': '/media/dbogdoll/Datasets/data_engine_rolling/data/plymouth_bishop_2023-03-13T14:56:11.395Z.jpg',\n",
      "    'tags': [],\n",
      "    'metadata': None,\n",
      "    'created_at': datetime.datetime(2024, 11, 14, 18, 39, 24, 355000),\n",
      "    'last_modified_at': datetime.datetime(2024, 11, 14, 18, 39, 24, 355000),\n",
      "    'sensor': 'plymouth_bishop',\n",
      "    'timestamp': datetime.datetime(2023, 3, 13, 14, 56, 11, 395000),\n",
      "}>]\n"
     ]
    }
   ],
   "source": [
    "def create_v51_dataset(name=\"Data Engine Rolling Dataset\", delete_old_dataset=False):\n",
    "\n",
    "    # Create the dataset\n",
    "    dataset = fo.Dataset(name = name, overwrite=delete_old_dataset)\n",
    "\n",
    "    dataset.add_dir(\n",
    "        dataset_dir=storage_target_root,\n",
    "        dataset_type=fo.types.FiftyOneDataset,\n",
    "        progress=True,\n",
    "    )\n",
    "\n",
    "    # View summary info about the dataset\n",
    "    print(dataset)\n",
    "\n",
    "    # Print the first few samples in the dataset\n",
    "    print(dataset.head())\n",
    "    return dataset\n",
    "\n",
    "v51_dataset = create_v51_dataset(delete_old_dataset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metadata...\n",
      " 100% |█████████████| 27096/27096 [1.4s elapsed, 0s remaining, 24.7K samples/s]         \n"
     ]
    }
   ],
   "source": [
    "NUM_WORKERS=32\n",
    "v51_dataset.compute_metadata(num_workers=NUM_WORKERS, progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:5151/?notebook=True&subscription=ed42bcef-9802-4c4d-8a8e-05339c985aaf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x731c92ca3c20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "session = fo.launch_app(v51_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
