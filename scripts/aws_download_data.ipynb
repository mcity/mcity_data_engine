{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import datetime\n",
    "import pytz\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import wandb\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import shutil\n",
    "import json\n",
    "import base64\n",
    "import fiftyone as fo\n",
    "import pkg_resources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime.datetime(2023, 11, 19)    # year, month, day   \n",
    "end_date = datetime.datetime(2023, 11, 25)      # 25 November 2023: Michigan vs Ohio\n",
    "cameras_excluded = []\n",
    "\n",
    "TEST_RUN = True\n",
    "\n",
    "# Path to download the data\n",
    "storage_target_root = \"/media/dbogdoll/Datasets/data_engine_rolling\"\n",
    "data_target = os.path.join(storage_target_root, \"data\")\n",
    "os.makedirs(data_target, exist_ok=True)\n",
    "\n",
    "log_target = os.path.join(storage_target_root, \"logs\")\n",
    "os.makedirs(log_target, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to AWS\n",
    "with open(\"/home/dbogdoll/mcity_data_engine/.secret\", \"r\") as file:\n",
    "    for line in file:\n",
    "        key, value = line.strip().split(\"=\")\n",
    "        os.environ[key] = value\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    aws_access_key_id=os.environ.get(\"AWS_ACCESS_KEY_ID\", None),\n",
    "    aws_secret_access_key=os.environ.get(\"AWS_SECRET_ACCESS_KEY\", None),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cameras = {\n",
    "    \"Geddes_Huron_1\",\n",
    "    \"Geddes_Huron_2\",\n",
    "    \"Huron_Plymouth_1\",\n",
    "    \"Huron_Plymouth_2\",\n",
    "    \"Main_stadium_1\",\n",
    "    \"Main_stadium_2\",\n",
    "    \"Plymouth_Beal\",\n",
    "    \"Plymouth_Bishop\",\n",
    "    \"Plymouth_EPA\",\n",
    "    \"Plymouth_Georgetown\",\n",
    "    \"State_Ellsworth_NE\",\n",
    "    \"State_Ellsworth_NW\",\n",
    "    \"State_Ellsworth_SE\",\n",
    "    \"State_Ellsworth_SW\",\n",
    "    \"Fuller_Fuller_CT\",\n",
    "    \"Fuller_Glazier_1\",\n",
    "    \"Fuller_Glazier_2\",\n",
    "    \"Fuller_Glen\",\n",
    "    \"Dexter_Maple_1\",\n",
    "    \"Dexter_Maple_2\",\n",
    "    \"Hubbard_Huron_1\",\n",
    "    \"Hubbard_Huron_2\",\n",
    "    \"Maple_Miller_1\",\n",
    "    \"Maple_Miller_2\",    \n",
    "}\n",
    "\n",
    "cameras_dict = {camera.lower(): {} for camera in cameras}\n",
    "\n",
    "for id, camera in enumerate(cameras_dict):\n",
    "    if camera not in cameras_excluded:\n",
    "        cameras_dict[camera][\"id\"] = id\n",
    "        cameras_dict[camera][\"aws-sources\"] = {}\n",
    "\n",
    "print(f\"Processed {len(cameras_dict)} cameras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_aws_result(result):\n",
    "    if 'CommonPrefixes' in result:\n",
    "        folders = [prefix['Prefix'] for prefix in result['CommonPrefixes']] # Get list of folders from AWS response\n",
    "        return folders\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_sources = {\n",
    "    \"sip-sensor-data\":[\"\"],\n",
    "    \"sip-sensor-data2\":[\"wheeler1/\",\"wheeler2/\"],\n",
    "}\n",
    "\n",
    "for bucket in tqdm(aws_sources, desc=\"Processing AWS sources\"):\n",
    "    for folder in aws_sources[bucket]:\n",
    "        # Get and pre-process AWS data\n",
    "        result = s3.list_objects_v2(Bucket=bucket, Prefix=folder, Delimiter='/')\n",
    "        folders = process_aws_result(result)\n",
    "\n",
    "        #Align folder names with camera names\n",
    "        folders_aligned = [re.sub(r'(?<!_)(\\d)', r'_\\1', folder.lower().rstrip('/')) for folder in folders] # Align varying AWS folder names with camera names\n",
    "        folders_aligned = [folder.replace('fullerct', 'fuller_ct') for folder in folders_aligned]  # Replace \"fullerct\" with \"fuller_ct\" to align with camera names\n",
    "        folders_aligned = [folder.replace('fullser', 'fuller') for folder in folders_aligned]  # Fix \"fullser\" typo in AWS\n",
    "\n",
    "        # Check cameras for AWS sources\n",
    "        if folders:\n",
    "            for camera_name in cameras_dict:\n",
    "                for folder_name, folder_name_aligned in zip(folders, folders_aligned):\n",
    "                    if camera_name in folder_name_aligned and \"gs_\" in folder_name_aligned: # gs_ is the prefix used in AWS\n",
    "                        aws_source = f\"{bucket}/{folder_name}\"\n",
    "                        if aws_source not in cameras_dict[camera_name][\"aws-sources\"]:\n",
    "                            cameras_dict[camera_name][\"aws-sources\"][aws_source] = {}\n",
    "        else:\n",
    "            print(f\"AWS did not return a list of folders for {bucket}/{folder}\")\n",
    "            print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_data_test():\n",
    "    \"Select a single test file to download for each AWS source\"\n",
    "    n_cameras = 0\n",
    "    n_aws_sources = 0\n",
    "    n_files_to_download = 0\n",
    "    download_size_bytes = 0\n",
    "    for camera in cameras_dict:\n",
    "        n_cameras += 1\n",
    "        for aws_source in cameras_dict[camera][\"aws-sources\"]:\n",
    "            n_aws_sources += 1\n",
    "            file_downloaded_test = False\n",
    "            bucket = aws_source.split(\"/\")[0]\n",
    "            prefix_camera = \"/\".join(aws_source.split(\"/\")[1:])\n",
    "            result = s3.list_objects_v2(Bucket=bucket, Prefix=prefix_camera, Delimiter='/')\n",
    "            folders_day = process_aws_result(result)        # Each folder represents a day\n",
    "            for folder_day in folders_day:\n",
    "                date = folder_day.split('/')[-2]\n",
    "                cameras_dict[camera][\"aws-sources\"][aws_source][date] = {}\n",
    "                result = s3.list_objects_v2(Bucket=bucket, Prefix=folder_day, Delimiter='/')\n",
    "                folders_hour = process_aws_result(result)    # Each folder represents an hour\n",
    "                for folder_hour in folders_hour:\n",
    "                    result = s3.list_objects_v2(Bucket=bucket, Prefix=folder_hour, Delimiter='/')\n",
    "                    files = result[\"Contents\"]\n",
    "                    for file in files:\n",
    "                        n_files_to_download += 1\n",
    "                        download_size_bytes += file[\"Size\"]\n",
    "                        file_name = os.path.basename(file[\"Key\"])\n",
    "                        cameras_dict[camera][\"aws-sources\"][aws_source][date][file_name] = {}\n",
    "                        cameras_dict[camera][\"aws-sources\"][aws_source][date][file_name][\"key\"] = file[\"Key\"]\n",
    "                        cameras_dict[camera][\"aws-sources\"][aws_source][date][file_name][\"size\"] = file[\"Size\"]\n",
    "                        cameras_dict[camera][\"aws-sources\"][aws_source][date][file_name][\"date\"] = file[\"LastModified\"].strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                        file_downloaded_test = True\n",
    "                        print(f\"{aws_source} : {file_name}\")\n",
    "                        break # for file in files:\n",
    "                    if file_downloaded_test:\n",
    "                        break   # for folder_hour in folders_hour:\n",
    "                if file_downloaded_test:\n",
    "                    break   # for folder_day in folders_day:\n",
    "\n",
    "    log_file_path = os.path.join(\"cameras_dict.json\")\n",
    "    with open(log_file_path, 'w') as json_file:\n",
    "        json.dump(cameras_dict, json_file, indent=4)  \n",
    "\n",
    "    print(f\"Found {n_cameras} cameras\")\n",
    "    print(f\"Found {n_aws_sources} AWS sources\")\n",
    "    print(f\"Found {n_files_to_download} files to download\")\n",
    "    return n_files_to_download, download_size_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_data():\n",
    "    n_files_to_download = 0\n",
    "    download_size_bytes = 0\n",
    "    for camera in tqdm(cameras_dict, desc=\"Looking for data entries in range\"):\n",
    "        for aws_source in cameras_dict[camera][\"aws-sources\"]:\n",
    "            bucket = aws_source.split(\"/\")[0]\n",
    "            prefix_camera = \"/\".join(aws_source.split(\"/\")[1:])\n",
    "            result = s3.list_objects_v2(Bucket=bucket, Prefix=prefix_camera, Delimiter='/')\n",
    "            folders_day = process_aws_result(result)        # Each folder represents a day\n",
    "            for folder_day in folders_day:\n",
    "                date = folder_day.split('/')[-2]\n",
    "                timestamp = datetime.datetime.strptime(date, '%Y-%m-%d')\n",
    "                if start_date <= timestamp <= end_date:     # Only collect data within the date range\n",
    "                    cameras_dict[camera][\"aws-sources\"][aws_source][date] = {}\n",
    "                    result = s3.list_objects_v2(Bucket=bucket, Prefix=folder_day, Delimiter='/')\n",
    "                    folders_hour = process_aws_result(result)    # Each folder represents an hour\n",
    "                    for folder_hour in folders_hour:\n",
    "                        result = s3.list_objects_v2(Bucket=bucket, Prefix=folder_hour, Delimiter='/')\n",
    "                        files = result[\"Contents\"]\n",
    "                        for file in files:\n",
    "                            n_files_to_download += 1\n",
    "                            download_size_bytes += file[\"Size\"]\n",
    "                            file_name = os.path.basename(file[\"Key\"])\n",
    "                            cameras_dict[camera][\"aws-sources\"][aws_source][date][file_name] = {}\n",
    "                            cameras_dict[camera][\"aws-sources\"][aws_source][date][file_name][\"key\"] = file[\"Key\"]\n",
    "                            cameras_dict[camera][\"aws-sources\"][aws_source][date][file_name][\"size\"] = file[\"Size\"]\n",
    "                            cameras_dict[camera][\"aws-sources\"][aws_source][date][file_name][\"date\"] = file[\"LastModified\"].strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    print(f\"Found {n_files_to_download} files to download\")\n",
    "    return n_files_to_download, download_size_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_RUN:\n",
    "    n_files_to_download, download_size_bytes = select_data_test()\n",
    "else:\n",
    "    n_files_to_download, download_size_bytes = select_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safety_checks():\n",
    "    # Safety checks prior to downloading\n",
    "    passed_checks = True\n",
    "\n",
    "    # Check if each camera was assigned at least one AWS source\n",
    "    for camera in cameras_dict:\n",
    "        if len(cameras_dict[camera][\"aws-sources\"]) == 0:\n",
    "            print(f\"Camera {camera} has no AWS sources\")\n",
    "            passed_checks = False\n",
    "\n",
    "    # Check if the total size of the data to download is within the limit\n",
    "    MAX_SIZE_TB = 1.5\n",
    "    total_size_tb = download_size_bytes / (1024 ** 4)\n",
    "    if total_size_tb > MAX_SIZE_TB:\n",
    "        print(f\"Total size {total_size_tb} TB exceeds {MAX_SIZE_TB} TB limit\")\n",
    "        passed_checks = False\n",
    "    else:\n",
    "        print(f\"Total size {total_size_tb} TB is within {MAX_SIZE_TB} TB limit\")\n",
    "\n",
    "    # Check if the date range is within the limit\n",
    "    MAX_DAYS = 7\n",
    "    days_delta = (end_date - start_date).days + 1 # +1 to include the end date\n",
    "    if (days_delta) > MAX_DAYS:\n",
    "        print(f\"Date range of {days_delta} days exceeds {MAX_DAYS} days limit\")\n",
    "        passed_checks = False\n",
    "    else:\n",
    "        print(f\"Date range of {days_delta} days is within {MAX_DAYS} days limit\")\n",
    "    return passed_checks\n",
    "\n",
    "passed_checks = safety_checks()\n",
    "print(passed_checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n",
    "def download_data(delete_old_data=False):\n",
    "    mb_per_s_list = []\n",
    "    \n",
    "    if passed_checks:\n",
    "        download_successful = True\n",
    "\n",
    "        if delete_old_data:\n",
    "            try:\n",
    "                shutil.rmtree(data_target)\n",
    "            except:\n",
    "                pass\n",
    "            os.makedirs(data_target) \n",
    "\n",
    "        step = 0\n",
    "        writer = SummaryWriter(log_dir=\"logs/download/s3\")\n",
    "        download_started = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        with tqdm(desc=\"Downloading data\", total=n_files_to_download) as pbar:\n",
    "            for camera in cameras_dict:\n",
    "                for aws_source in cameras_dict[camera][\"aws-sources\"]:\n",
    "                    bucket = aws_source.split(\"/\", 1)[0]\n",
    "                    for date in cameras_dict[camera][\"aws-sources\"][aws_source]:\n",
    "                        for file in cameras_dict[camera][\"aws-sources\"][aws_source][date]:\n",
    "                            time_start = time.time()\n",
    "                            \n",
    "                            # AWS S3 Download\n",
    "                            file_name = os.path.basename(file)\n",
    "                            key = cameras_dict[camera][\"aws-sources\"][aws_source][date][file_name][\"key\"]\n",
    "                            target = os.path.join(data_target, file_name)\n",
    "                            s3.download_file(bucket, key, target)\n",
    "                            \n",
    "                            # Calculate duration and GB/s\n",
    "                            file_size_mb = cameras_dict[camera][\"aws-sources\"][aws_source][date][file_name][\"size\"] / (1024 ** 2)\n",
    "                            time_end = time.time()\n",
    "                            duration = time_end - time_start\n",
    "                            mb_per_s = file_size_mb / duration\n",
    "                            mb_per_s_list.append(mb_per_s)\n",
    "                            \n",
    "                            # Update stats\n",
    "                            step += 1\n",
    "                            pbar.update(1)\n",
    "                            writer.add_scalar(\"download/mb_per_second\", mb_per_s, step)\n",
    "        writer.close()\n",
    "\n",
    "        # Check if all files were downloaded properly\n",
    "        downloaded_files = os.listdir(data_target)\n",
    "\n",
    "        for camera in tqdm(cameras_dict, desc=\"Checking downloaded data\"):\n",
    "            for aws_source in cameras_dict[camera][\"aws-sources\"]:\n",
    "                for date in cameras_dict[camera][\"aws-sources\"][aws_source]:\n",
    "                    for file in cameras_dict[camera][\"aws-sources\"][aws_source][date]:\n",
    "                        file_name = os.path.basename(file)\n",
    "                        if file not in downloaded_files:\n",
    "                            download_successful = False\n",
    "                            print(f\"File {file} was not downloaded properly\")\n",
    "                        else:\n",
    "                            cameras_dict[camera][\"aws-sources\"][aws_source][date][file_name][\"download_successful\"] = True\n",
    "        \n",
    "        log = {}\n",
    "        log[\"selection_start_date\"] = start_date.strftime(\"%Y-%m-%d\")\n",
    "        log[\"selection_end_date\"] = end_date.strftime(\"%Y-%m-%d\")\n",
    "        log[\"old_files_deleted\"] = delete_old_data\n",
    "        log[\"test_run\"] = TEST_RUN\n",
    "        log[\"n_files_to_download\"] = n_files_to_download\n",
    "        log[\"download_size_tb\"] = download_size_bytes / (1024 ** 4)\n",
    "        log[\"download_started\"] = download_started\n",
    "        log[\"download_ended\"] = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        log[\"download_speed_avg_mbs\"] = sum(mb_per_s_list) / len(mb_per_s_list)\n",
    "        log[\"data\"] = cameras_dict\n",
    "        log_name = download_started.replace(\" \", \"_\").replace(\":\", \"_\") + \".json\"\n",
    "        log_file_path = os.path.join(log_target, log_name)\n",
    "        with open(log_file_path, 'w') as json_file:\n",
    "            json.dump(log, json_file, indent=4)  \n",
    "\n",
    "    else:\n",
    "        download_successful = False\n",
    "        print(\"Safety checks failed. Not downloading data\")\n",
    "\n",
    "    return download_successful\n",
    "\n",
    "run = wandb.init(\n",
    "    name=f\"{start_date.strftime('%Y%m%d')}_to_{end_date.strftime('%Y%m%d')}\",\n",
    "    sync_tensorboard=True,\n",
    "    group=\"S3\",\n",
    "    job_type=\"download\",\n",
    "    project=\"Data Engine Download\",\n",
    ")\n",
    "\n",
    "download_successful = download_data(delete_old_data=True)\n",
    "print(download_successful)\n",
    "run.finish(exit_code=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_v51_metadata():\n",
    "\n",
    "    # Define sample fields\n",
    "    sample_fields = [\n",
    "        {\n",
    "            \"name\": \"filepath\",\n",
    "            \"ftype\": \"fiftyone.core.fields.StringField\",\n",
    "            \"embedded_doc_type\": None,\n",
    "            \"subfield\": None,\n",
    "            \"fields\": [],\n",
    "            \"db_field\": \"filepath\",\n",
    "            \"description\": None,\n",
    "            \"info\": None,\n",
    "            \"read_only\": True,\n",
    "            \"created_at\": {\n",
    "                \"$date\": \"2024-11-14T15:24:21.719Z\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"sensor\",\n",
    "            \"ftype\": \"fiftyone.core.fields.StringField\",\n",
    "            \"embedded_doc_type\": None,\n",
    "            \"subfield\": None,\n",
    "            \"fields\": [],\n",
    "            \"db_field\": \"sensor\",\n",
    "            \"description\": None,\n",
    "            \"info\": None,\n",
    "            \"read_only\": True,\n",
    "            \"created_at\": {\n",
    "                \"$date\": \"2024-11-14T15:24:21.719Z\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"timestamp\",\n",
    "            \"ftype\": \"fiftyone.core.fields.DateTimeField\",\n",
    "            \"embedded_doc_type\": None,\n",
    "            \"subfield\": None,\n",
    "            \"fields\": [],\n",
    "            \"db_field\": \"timestamp\",\n",
    "            \"description\": None,\n",
    "            \"info\": None,\n",
    "            \"read_only\": True,\n",
    "            \"created_at\": {\n",
    "                \"$date\": \"2024-11-14T15:24:21.719Z\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Get version of current V51 package\n",
    "    package_name = 'fiftyone'\n",
    "    version = pkg_resources.get_distribution(package_name).version\n",
    "    version_str = str(version)\n",
    "\n",
    "    v51_metadata = {}\n",
    "    v51_metadata[\"name\"] = \"Data Engine Rolling Dataset\"\n",
    "    v51_metadata[\"version\"] = version_str\n",
    "    v51_metadata[\"sample_fields\"] = sample_fields\n",
    "\n",
    "    file_path = os.path.join(storage_target_root, \"metadata.json\")\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(v51_metadata, json_file)\n",
    "\n",
    "set_v51_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack data\n",
    "# Support V51 dataset structure https://docs.voxel51.com/user_guide/dataset_creation/datasets.html#fiftyonedataset\n",
    "\n",
    "def unpack_data():\n",
    "    v51_samples = {}\n",
    "    v51_samples_array = []\n",
    "    if download_successful:\n",
    "        for camera in tqdm(cameras_dict, desc=\"Unpacking data\"):\n",
    "            for aws_source in cameras_dict[camera][\"aws-sources\"]:\n",
    "                for date in cameras_dict[camera][\"aws-sources\"][aws_source]:\n",
    "                    for file in cameras_dict[camera][\"aws-sources\"][aws_source][date]:\n",
    "                        file_name = os.path.basename(file)\n",
    "                        file_path = os.path.join(data_target, file)\n",
    "                        unpacking_successful = True\n",
    "                        with open(file_path, 'r') as file:\n",
    "                            for line in file:\n",
    "                                try:\n",
    "                                    data = json.loads(line)\n",
    "                                    v51_sample = {}\n",
    "                                    if \"time\" in data and \"data\" in data:\n",
    "                                        # Get data\n",
    "                                        timestamp = data.get(\"time\")\n",
    "                                        image_base64 = data.get(\"data\")\n",
    "\n",
    "                                        # Get timestamp\n",
    "                                        time_obj = datetime.datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S.%f')\n",
    "                                        formatted_time = time_obj.strftime('%Y-%m-%dT%H:%M:%S.%f')[:-3] + 'Z'\n",
    "                                        \n",
    "                                    elif \"image\" in data and \"sensor_name\" in data and \"event_timestamp\" in data:\n",
    "                                        # Get data\n",
    "                                        image_base64 = data.get(\"image\")\n",
    "                                        sensor_name = data.get(\"sensor_name\")\n",
    "                                        timestamp = data.get(\"event_timestamp\")\n",
    "                                        \n",
    "                                        # Get timestamps in UTC and Michigan time\n",
    "                                        utc_time = datetime.datetime.fromtimestamp(timestamp, tz=datetime.timezone.utc)\n",
    "                                        michigan_tz = pytz.timezone('America/Detroit')\n",
    "                                        michigan_time = utc_time.astimezone(michigan_tz)\n",
    "                                        formatted_time = michigan_time.strftime('%Y-%m-%dT%H:%M:%S.%f')[:-3] + 'Z'\n",
    "                                        # TODO Make sure that the conversion to Michigan time is correct\n",
    "                                    else:\n",
    "                                        unpacking_successful = False\n",
    "                                        print(f\"Format cannot be processed: {data}\")\n",
    "                                        continue\n",
    "                                    \n",
    "                                    if image_base64 and formatted_time:\n",
    "                                        # Decode the base64 image data\n",
    "                                        image_data = base64.b64decode(image_base64)\n",
    "\n",
    "                                        # File paths\n",
    "                                        image_filename = f\"{camera}_{formatted_time}.jpg\"\n",
    "                                        output_path = os.path.join(data_target, image_filename)\n",
    "\n",
    "                                        # Ensure correct timestamp format\n",
    "                                        milliseconds = formatted_time.split('.')[1][:3].ljust(3, '0')\n",
    "                                        formatted_time = formatted_time.split('.')[0] + '.' + milliseconds + 'Z'\n",
    "                                        iso8601_regex = re.compile(r'^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}(\\.\\d{3})?Z$')\n",
    "                                        iso8601_conform = bool(iso8601_regex.match(formatted_time))\n",
    "                                        if not iso8601_conform:\n",
    "                                            print(f\"Timestamp does not conform to ISO8601: {formatted_time}\")\n",
    "\n",
    "                                        # Prepare import with V51\n",
    "                                        v51_sample[\"filepath\"] = output_path\n",
    "                                        v51_sample[\"sensor\"] = camera\n",
    "                                        v51_sample[\"timestamp\"] = {\"$date\": formatted_time}\n",
    "                                        v51_samples_array.append(v51_sample) \n",
    "\n",
    "                                        # Save the decoded image data as a JPEG\n",
    "                                        with open(output_path, 'wb') as image_file:\n",
    "                                            image_file.write(image_data)\n",
    "                                    else:\n",
    "                                        unpacking_successful = False\n",
    "                                        print(f\"There was an issue during file processing of {file}\")\n",
    "                                        continue\n",
    "\n",
    "                                except json.JSONDecodeError as e:\n",
    "                                    unpacking_successful = False\n",
    "                                    print(f\"Error decoding JSON: {e}\")\n",
    "                        \n",
    "                        # Delete the original file if unpacking was successful\n",
    "                        if unpacking_successful:\n",
    "                            cameras_dict[camera][\"aws-sources\"][aws_source][date][file_name][\"unpack_successful\"] = True\n",
    "                            os.remove(file_path)\n",
    "\n",
    "    # Store V51 metadata files\n",
    "    v51_samples[\"samples\"] = v51_samples_array\n",
    "    file_path = os.path.join(storage_target_root, \"samples.json\")\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(v51_samples, json_file)\n",
    "\n",
    "unpack_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_v51_dataset(name=\"Data Engine Rolling Dataset\", delete_old_dataset=False, NUM_WORKERS=32):\n",
    "\n",
    "    # Create the dataset\n",
    "    dataset = fo.Dataset(name = name, overwrite=delete_old_dataset)\n",
    "\n",
    "    dataset.add_dir(\n",
    "        dataset_dir=storage_target_root,\n",
    "        dataset_type=fo.types.FiftyOneDataset,\n",
    "        progress=True,\n",
    "    )\n",
    "\n",
    "    dataset.compute_metadata(num_workers=NUM_WORKERS, progress=True)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "v51_dataset = create_v51_dataset(delete_old_dataset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = {}\n",
    "log[\"selection_start_date\"] = start_date.strftime(\"%Y-%m-%d\")\n",
    "log[\"selection_end_date\"] = end_date.strftime(\"%Y-%m-%d\")\n",
    "log[\"old_files_deleted\"] = delete_old_data\n",
    "log[\"test_run\"] = TEST_RUN\n",
    "log[\"n_files_to_download\"] = n_files_to_download\n",
    "log[\"download_size_tb\"] = download_size_bytes / (1024 ** 4)\n",
    "log[\"download_started\"] = download_started\n",
    "log[\"download_ended\"] = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "log[\"download_speed_avg_mbs\"] = sum(mb_per_s_list) / len(mb_per_s_list)\n",
    "log[\"data\"] = cameras_dict\n",
    "log_name = download_started.replace(\" \", \"_\").replace(\":\", \"_\") + \".json\"\n",
    "log_file_path = os.path.join(log_target, log_name)\n",
    "with open(log_file_path, 'w') as json_file:\n",
    "    json.dump(log, json_file, indent=4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(v51_dataset)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
