{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import fiftyone as fo\n",
    "\n",
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "from utils.data_loader import (\n",
    "    FiftyOneTorchDatasetCOCO,\n",
    "    FiftyOneTorchDatasetCOCOFilepaths,\n",
    "    TorchToHFDatasetCOCO,\n",
    ")\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = {}\n",
    "with open(\"/home/dbogdoll/mcity_data_engine/.secret\", \"r\") as file:\n",
    "    for line in file:\n",
    "        key, value = line.strip().split(\"=\")\n",
    "        tokens[key] = value\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = tokens[\"HF_TOKEN\"]\n",
    "\n",
    "try:\n",
    "    dataset_v51 = load_from_hub(\"dbogdollumich/mcity_fisheye_v51\")\n",
    "except:\n",
    "    dataset_v51 = fo.load_dataset(\"dbogdollumich/mcity_fisheye_v51\")\n",
    "\n",
    "print(dataset_v51.stats)\n",
    "print(dataset_v51.summary)\n",
    "print(dataset_v51.name)\n",
    "print(len(dataset_v51))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset_v51.first()\n",
    "print(sample)\n",
    "sample2 = dataset_v51[sample.filepath]\n",
    "\n",
    "ground_truth = sample[\"ground_truth\"]\n",
    "print(ground_truth)\n",
    "print(len(ground_truth[\"detections\"]))\n",
    "\n",
    "for detection in ground_truth[\"detections\"]:\n",
    "    print(detection)\n",
    "    print(detection.bounding_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "# dataset_v51 = dataset_v51.take(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dataset_filepaths = FiftyOneTorchDatasetCOCOFilepaths(dataset_v51)\n",
    "torch_dataloader_filepaths = DataLoader(\n",
    "    torch_dataset_filepaths,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=32,\n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    "    prefetch_factor=2,\n",
    "    persistent_workers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dataset_managed = FiftyOneTorchDatasetCOCO(dataset_v51)\n",
    "torch_dataloader_managed = DataLoader(\n",
    "    torch_dataset_managed,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=32,\n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    "    prefetch_factor=2,\n",
    "    collate_fn=lambda batch: list(zip(*batch)),\n",
    ")\n",
    "\n",
    "print(torch_dataset_managed.get_classes())\n",
    "print(torch_dataset_managed.get_splits())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = TorchToHFDatasetCOCO(torch_dataset_managed)\n",
    "hf_dataset = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.utils.coco as fouc\n",
    "import torch\n",
    "\n",
    "\n",
    "def target_batch_to_detections(targets, dataset, gt_field=\"ground_truth\"):\n",
    "    classes = dataset_v51.default_classes\n",
    "    batch_targets = []\n",
    "    samples = [dataset[filepath] for filepath in targets]\n",
    "    for sample in samples:\n",
    "        metadata = sample.metadata\n",
    "        id = sample.id\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        area = []\n",
    "        iscrowd = []\n",
    "        for detection in sample[gt_field][\"detections\"]:\n",
    "            category_id = classes.index(detection.label)\n",
    "            coco_obj = fouc.COCOObject.from_label(\n",
    "                detection,\n",
    "                metadata,\n",
    "                category_id=category_id,\n",
    "            )\n",
    "            x, y, w, h = coco_obj.bbox\n",
    "            boxes.append([x, y, w, h])\n",
    "            labels.append(coco_obj.category_id)\n",
    "            area.append(coco_obj.area)\n",
    "            iscrowd.append(coco_obj.iscrowd)\n",
    "\n",
    "        target_dict = {}\n",
    "        target_dict[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        target_dict[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        target_dict[\"image_id\"] = id\n",
    "        target_dict[\"area\"] = torch.as_tensor(area, dtype=torch.float32)\n",
    "        target_dict[\"iscrowd\"] = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
    "        batch_targets.append(target_dict)\n",
    "    return batch_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForZeroShotObjectDetection,\n",
    ")\n",
    "\n",
    "model_name = \"google/owlv2-base-patch16-ensemble\"\n",
    "texts = [\n",
    "    \"car\",\n",
    "    \"truck\",\n",
    "    \"bus\",\n",
    "    \"trailer\",\n",
    "    \"motorbike/cycler\",\n",
    "    \"pedestrian\",\n",
    "    \"van\",\n",
    "    \"pickup\",\n",
    "]\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(model_name).to(device)\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_classes = texts * batch_size\n",
    "tokenized_text = processor.tokenizer(\n",
    "    batch_classes, padding=\"max_length\", return_tensors=\"pt\"\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = len(dataset_v51)\n",
    "estimated_steps = n_samples / batch_size\n",
    "torch.cuda.empty_cache()\n",
    "steps = 0\n",
    "for batch_idx, (images, targets) in enumerate(torch_dataloader_managed):\n",
    "    print(batch_idx)\n",
    "    steps += 1\n",
    "print(\"Estimated steps: \", estimated_steps)\n",
    "print(\"Performed steps: \", steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "for batch_idx, (images, targets) in enumerate(torch_dataloader_filepaths):\n",
    "    targets = target_batch_to_detections(targets, dataset_v51)\n",
    "    target_sizes = [tuple(img.shape[1:]) for img in images]\n",
    "    inputs = processor(text=None, images=images, return_tensors=\"pt\").to(device)\n",
    "    inputs.update(tokenized_text)\n",
    "    with torch.amp.autocast(\"cuda\"):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "    results = processor.post_process_object_detection(\n",
    "        outputs=outputs,\n",
    "        threshold=0.2,\n",
    "        target_sizes=target_sizes,\n",
    "    )\n",
    "    for result, target in zip(results, targets):\n",
    "        boxes, scores, labels = result[\"boxes\"], result[\"scores\"], result[\"labels\"]\n",
    "\n",
    "        detections = []\n",
    "        for box, score, label in zip(boxes, scores, labels):\n",
    "\n",
    "            # Get image size (ID is stored in annotation)\n",
    "            sample = dataset_v51[target[\"image_id\"]]\n",
    "            img_width = sample.metadata.width\n",
    "            img_height = sample.metadata.height\n",
    "\n",
    "            # Convert bbox to V51 type\n",
    "            label = texts[label]\n",
    "            top_left_x = box[0].item() / img_width\n",
    "            top_left_y = box[1].item() / img_height\n",
    "            box_width = (box[2].item() - box[0].item()) / img_width\n",
    "            box_height = (box[3].item() - box[1].item()) / img_height\n",
    "\n",
    "            detection = fo.Detection(\n",
    "                label=label,\n",
    "                bounding_box=[\n",
    "                    top_left_x,\n",
    "                    top_left_y,\n",
    "                    box_width,\n",
    "                    box_height,\n",
    "                ],\n",
    "                confidence=score.item(),\n",
    "            )\n",
    "\n",
    "            detections.append(detection)\n",
    "\n",
    "        # Attach label to V51 dataset\n",
    "        sample = dataset_v51[target[\"image_id\"]]\n",
    "        sample[\"prediction\"] = fo.Detections(detections=detections)\n",
    "        sample.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "for batch_idx, (images, targets) in enumerate(torch_dataloader_managed):\n",
    "    target_sizes = [tuple(img.shape[1:]) for img in images]\n",
    "    inputs = processor(text=None, images=images, return_tensors=\"pt\").to(device)\n",
    "    inputs.update(tokenized_text)\n",
    "    with torch.amp.autocast(\"cuda\"):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "    results = processor.post_process_object_detection(\n",
    "        outputs=outputs,\n",
    "        threshold=0.2,\n",
    "        target_sizes=target_sizes,\n",
    "    )\n",
    "    for result, target in zip(results, targets):\n",
    "        boxes, scores, labels = result[\"boxes\"], result[\"scores\"], result[\"labels\"]\n",
    "\n",
    "        detections = []\n",
    "        for box, score, label, img_size in zip(boxes, scores, labels, target_sizes):\n",
    "\n",
    "            img_width = img_size[1]\n",
    "            img_height = img_size[0]\n",
    "\n",
    "            # Convert bbox to V51 type\n",
    "            label = texts[label]\n",
    "            top_left_x = box[0].item() / img_width\n",
    "            top_left_y = box[1].item() / img_height\n",
    "            box_width = (box[2].item() - box[0].item()) / img_width\n",
    "            box_height = (box[3].item() - box[1].item()) / img_height\n",
    "\n",
    "            detection = fo.Detection(\n",
    "                label=label,\n",
    "                bounding_box=[\n",
    "                    top_left_x,\n",
    "                    top_left_y,\n",
    "                    box_width,\n",
    "                    box_height,\n",
    "                ],\n",
    "                confidence=score.item(),\n",
    "            )\n",
    "\n",
    "            detections.append(detection)\n",
    "\n",
    "        # Attach label to V51 dataset\n",
    "        sample = dataset_v51[target[\"image_id\"]]\n",
    "        sample[\"prediction_managed\"] = fo.Detections(detections=detections)\n",
    "        sample.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filepaths: 10.51 with batch_size 8  \n",
    "Managed: 9.56 with batch_size 8 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
