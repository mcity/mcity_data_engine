{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image, ImageDraw\n",
    "import torch\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "import skimage.transform\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(\n",
    "    \"google/owlvit-base-patch32\"\n",
    ")\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "image = Image.open(\"/home/dbogdoll/mcity_data_engine/scripts/fisheye_test.png\").convert(\n",
    "    \"RGB\"\n",
    ")\n",
    "image_np = np.array(image).astype(np.float32) / 255.0\n",
    "\n",
    "# Calculate padding to make the image square\n",
    "h, w, _ = image_np.shape\n",
    "size = max(h, w)\n",
    "padding = ((0, size - h), (0, size - w), (0, 0))\n",
    "\n",
    "# Pad the image with gray pixels (value 0.5)\n",
    "image_padded = np.pad(image_np, padding, constant_values=0.5)\n",
    "\n",
    "# Resize the padded image to 768x768\n",
    "input_image = skimage.transform.resize(image_padded, (768, 768), anti_aliasing=True)\n",
    "\n",
    "# Convert back to PIL Image if needed\n",
    "image = Image.fromarray((input_image * 255).astype(np.uint8))\n",
    "\n",
    "# Print the resolution and padding information\n",
    "print(f\"Original resolution: {h}x{w}\")\n",
    "print(f\"Padded resolution: {image_padded.shape[0]}x{image_padded.shape[1]}\")\n",
    "print(f\"Final resolution: {input_image.shape[0]}x{input_image.shape[1]}\")\n",
    "print(f\"Padding added: height={size - h}, width={size - w}\")\n",
    "\n",
    "# image = Image.open(\"/home/dbogdoll/mcity_data_engine/scripts/fisheye_small.png\").convert(\n",
    "#    \"RGB\"\n",
    "# )\n",
    "\n",
    "texts = [\n",
    "    \"car\",\n",
    "    \"truck\",\n",
    "    \"bus\",\n",
    "]\n",
    "tokenized_texts = processor.tokenizer(texts, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "# Preprocess image\n",
    "inputs = processor.image_processor(text=None, images=image, return_tensors=\"pt\")\n",
    "\n",
    "# Update inputs with tokenized text fields\n",
    "inputs.update(tokenized_texts)\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Target image sizes (height, width) to rescale box predictions [batch_size, 2]\n",
    "target_sizes = torch.Tensor([image.size[::-1]])\n",
    "print(target_sizes)\n",
    "# Convert outputs (bounding boxes and class logits) to COCO API\n",
    "results = processor.post_process_object_detection(\n",
    "    outputs=outputs, threshold=0.1, target_sizes=target_sizes\n",
    ")\n",
    "\n",
    "i = 0  # Retrieve predictions for the first image for the corresponding text queries\n",
    "text = texts[i]\n",
    "boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n",
    "\n",
    "# Print detected objects and rescaled box coordinates\n",
    "for box, score, label in zip(boxes, scores, labels):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    print(\n",
    "        f\"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw bounding boxes on the image\n",
    "draw = ImageDraw.Draw(image)\n",
    "for box, label in zip(boxes, labels):\n",
    "    draw.rectangle(box.tolist(), outline=\"red\", width=2)\n",
    "    draw.text((box[0], box[1]), text[label], fill=\"red\")\n",
    "\n",
    "# Display the image\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    AutoModelForZeroShotObjectDetection,\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Load the processor and model\n",
    "processor = AutoProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(\n",
    "    \"google/owlvit-base-patch32\"\n",
    ").to(device)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load images\n",
    "urls = [\n",
    "    \"http://images.cocodataset.org/val2017/000000039769.jpg\",\n",
    "    \"https://farm4.staticflickr.com/3154/2913892833_c740d2ca64_z.jpg\",\n",
    "]\n",
    "images_web = [Image.open(requests.get(url, stream=True).raw) for url in urls]\n",
    "\n",
    "images = [\n",
    "    Image.open(\"/home/dbogdoll/mcity_data_engine/scripts/fisheye_test.png\").convert(\n",
    "        \"RGB\"\n",
    "    ),\n",
    "    Image.open(\"/home/dbogdoll/mcity_data_engine/scripts/fisheye_test.png\").convert(\n",
    "        \"RGB\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "images_new = []\n",
    "for img in images:\n",
    "    base_width = 480\n",
    "    wpercent = base_width / float(img.size[0])\n",
    "    hsize = int((float(img.size[1]) * float(wpercent)))\n",
    "    img = img.resize((base_width, hsize), Image.Resampling.LANCZOS)\n",
    "    images_new.append(img)\n",
    "\n",
    "images = images_web + images_new\n",
    "print(images)\n",
    "\n",
    "# texts = [\"cat\", \"remote control\", \"dog\", \"traffic light\"]\n",
    "texts = [\"cat\", \"remote control\", \"bird\" \"car\", \"truck\", \"bus\", \"crosswalk\", \"sky\"]\n",
    "\n",
    "texts = texts * 4\n",
    "print(texts)\n",
    "\n",
    "# Process images with the processor\n",
    "inputs = processor(text=texts, images=images, return_tensors=\"pt\").to(device)\n",
    "print(inputs)\n",
    "# Forward pass\n",
    "with torch.amp.autocast(\"cuda\"):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "target_sizes = [x.size[::-1] for x in images]\n",
    "print(target_sizes)\n",
    "print(outputs)\n",
    "results = processor.post_process_object_detection(\n",
    "    outputs=outputs, threshold=0.1, target_sizes=target_sizes\n",
    ")\n",
    "\n",
    "\n",
    "print(results)\n",
    "for image, result in zip(images, results):\n",
    "    boxes, scores, labels = result[\"boxes\"], result[\"scores\"], result[\"labels\"]\n",
    "\n",
    "    # Draw bounding boxes on the image\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for box, label in zip(boxes, labels):\n",
    "        draw.rectangle(box.tolist(), outline=\"red\", width=2)\n",
    "        draw.text((box[0], box[1]), texts[label], fill=\"red\")\n",
    "\n",
    "    # Display the image\n",
    "    display(image)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
