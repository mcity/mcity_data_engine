{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "def load_mcity_fisheye_3_months_dev(dataset_info):\n",
    "    \"\"\"\n",
    "    Loads the Mcity Fisheye 3 months dataset based on the provided dataset information.\n",
    "\n",
    "    Args:\n",
    "        dataset_info (dict): A dictionary containing the following keys:\n",
    "            - \"name\" (str): The name of the dataset.\n",
    "            - \"local_path\" (str): The local path to the dataset directory.\n",
    "            - \"v51_type\" (str): The type of the dataset, corresponding to a type in `fo.types`.\n",
    "            - \"v51_splits\" (list): A list of dataset splits to be loaded.\n",
    "\n",
    "    Returns:\n",
    "        fo.Dataset: The loaded dataset object.\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If any of the required keys are missing in `dataset_info`.\n",
    "        AttributeError: If `v51_type` does not correspond to a valid type in `fo.types`.\n",
    "    \"\"\"\n",
    "    dataset_name = \"mcity_3_months_dev\"\n",
    "    dataset_dir = dataset_info[\"local_path\"]\n",
    "    dataset_type = getattr(fo.types, dataset_info[\"v51_type\"])\n",
    "    dataset_splits = dataset_info[\"v51_splits\"]  # Use all available splits\n",
    "\n",
    "    if dataset_name in fo.list_datasets():\n",
    "        fo.delete_dataset(dataset_name)\n",
    "    \n",
    "    dataset = fo.Dataset(dataset_name)\n",
    "    for split in dataset_splits:\n",
    "        dataset.add_dir(\n",
    "            dataset_dir=dataset_dir,\n",
    "            dataset_type=dataset_type,\n",
    "            split=split,\n",
    "            tags=split,\n",
    "        )\n",
    "    dataset.compute_metadata(num_workers=8)\n",
    "\n",
    "    dataset.persistent = False  # https://docs.voxel51.com/user_guide/using_datasets.html#dataset-persistence\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def get_metadata(dataset):\n",
    "    # Add dataset specific metedata based on filename\n",
    "    view = dataset.view()\n",
    "    for sample in tqdm(view, desc=\"Deriving metadata from filenames\"):  # https://docs.voxel51.com/api/fiftyone.core.sample.html\n",
    "        metadata = process_mcity_fisheye_3_months_filename_dev(sample[\"filepath\"])\n",
    "        sample[\"location\"] = metadata[\"location\"]\n",
    "        sample[\"name\"] = metadata[\"name\"]\n",
    "        sample[\"timestamp\"] = metadata[\"timestamp\"]\n",
    "        sample.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "def process_mcity_fisheye_3_months_filename_dev(filename):\n",
    "    \"\"\"\n",
    "    Processes a given filename to extract metadata including location, name, and timestamp.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The full path or name of the file to be processed.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the following keys:\n",
    "            - 'filename' (str): The base name of the file.\n",
    "            - 'location' (str or None): The location extracted from the filename, if available.\n",
    "            - 'name' (str or None): The cleaned name extracted from the filename.\n",
    "            - 'timestamp' (datetime or None): The timestamp extracted from the filename, if available.\n",
    "\n",
    "    The function performs the following steps:\n",
    "        1. Extracts the base name of the file.\n",
    "        2. Searches for a known location within the filename.\n",
    "        3. Splits the filename into two parts based on the first occurrence of a 4-digit year.\n",
    "        4. Cleans up the first part to derive the name.\n",
    "        5. Extracts and parses the timestamp from the second part of the filename.\n",
    "    \"\"\"\n",
    "\n",
    "    filename = os.path.basename(filename)\n",
    "    #Works gridsmart_ne_stage_1_2021_05_02_labeled_2021-05-02_13-52-35-657986.jpg\n",
    "    #Error gs_Geddes_Huron1_2023-09-01 13-01-15-785124.jpg\n",
    "    \n",
    "    results = {\"filename\": filename, \"location\": None, \"name\": None, \"timestamp\": None}\n",
    "\n",
    "    available_locations = [\n",
    "        \"beal\",\n",
    "        \"bishop\",\n",
    "        \"georgetown\",\n",
    "        \"gridsmart_ne\",\n",
    "        \"gridsmart_nw\",\n",
    "        \"gridsmart_se\",\n",
    "        \"gridsmart_sw\",\n",
    "        \"Huron_Plymouth-Geddes\",\n",
    "        \"Main_stadium\",\n",
    "        \"gs_Geddes_Huron\",\n",
    "        \"gs_Huron_Plymouth\",\n",
    "        \"gs_Plymouth_Beal\",\n",
    "        \"gs_Plymouth_Georgetown\",\n",
    "        \"gs_Plymouth_Bishop\",\n",
    "        \"gs_Plymouth_EPA\"]\n",
    "\n",
    "    for location in available_locations:\n",
    "        if location in filename:\n",
    "            results[\"location\"] = location\n",
    "            break\n",
    "    \n",
    "    if results[\"location\"] is None:\n",
    "        logging.error(f\"Filename {filename} could not be assigned to a known location\")\n",
    "\n",
    "    # Split string into first and second part based on first 4 digit year number\n",
    "    match = re.search(r\"\\d{4}\", filename)\n",
    "    if match:\n",
    "        year_index = match.start()\n",
    "        part1 = filename[:year_index]\n",
    "        part2 = filename[year_index:]\n",
    "\n",
    "    # Cleanup first part\n",
    "    results[\"name\"] = re.sub(r\"[-_]+$\", \"\", part1)\n",
    "\n",
    "    # Extract timestamp from second part\n",
    "    match = re.search(r\"\\d{8}T\\d{6}|\\d{4}-\\d{2}-\\d{2}[_ ]\\d{2}-\\d{2}-\\d{2}\", part2)\n",
    "    if match:\n",
    "        extracted_timestamp = match.group(0)\n",
    "    \n",
    "        if re.match(r\"\\d{8}T\\d{6}\", extracted_timestamp):\n",
    "            results[\"timestamp\"] = datetime.strptime(extracted_timestamp, \"%Y%m%dT%H%M%S\")\n",
    "        elif re.match(r\"\\d{4}-\\d{2}-\\d{2}_\\d{2}-\\d{2}-\\d{2}\", extracted_timestamp):\n",
    "            results[\"timestamp\"] = datetime.strptime(extracted_timestamp, \"%Y-%m-%d_%H-%M-%S\")\n",
    "        elif re.match(r\"\\d{4}-\\d{2}-\\d{2} \\d{2}-\\d{2}-\\d{2}\", extracted_timestamp):\n",
    "            results[\"timestamp\"] = datetime.strptime(extracted_timestamp, \"%Y-%m-%d %H-%M-%S\")\n",
    "        else:\n",
    "            logging.error(\"Unknown timestamp format\")\n",
    "    else:\n",
    "        logging.error(\"No valid timestamp found in string\")\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████| 48568/48568 [1.7m elapsed, 0s remaining, 860.2 samples/s]      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:eta.core.utils: 100% |█████████████| 48568/48568 [1.7m elapsed, 0s remaining, 860.2 samples/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 744/744 [1.7s elapsed, 0s remaining, 447.0 samples/s]      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:eta.core.utils: 100% |█████████████████| 744/744 [1.7s elapsed, 0s remaining, 447.0 samples/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metadata...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.core.metadata:Computing metadata...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████| 49312/49312 [4.8s elapsed, 0s remaining, 10.2K samples/s]      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:eta.core.utils: 100% |█████████████| 49312/49312 [4.8s elapsed, 0s remaining, 10.2K samples/s]      \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from utils.dataset_loader import *\n",
    "\n",
    "SELECTED_DATASET = \"mcity_fisheye_3_months\"\n",
    "dataset_info = load_dataset_info(SELECTED_DATASET, config_path=\"/home/dbogdoll/mcity_data_engine/config/datasets.yaml\")\n",
    "dataset = load_mcity_fisheye_3_months_dev(dataset_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deriving metadata from filenames:   0%|          | 0/49312 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deriving metadata from filenames: 100%|██████████| 49312/49312 [01:12<00:00, 681.38it/s] \n"
     ]
    }
   ],
   "source": [
    "get_metadata(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
